{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c5ba4e",
   "metadata": {},
   "source": [
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <td style=\"width:20%; vertical-align:middle;\">\n",
    "      <img src=\"LogoUVG.png\" width=\"400\"/>\n",
    "    </td>\n",
    "    <td style=\"text-align:left; vertical-align:middle;\">\n",
    "      <h2 style=\"margin-bottom: 0;\">Universidad del Valle de Guatemala - UVG</h2>\n",
    "      <h3 style=\"margin-top: 0;\">Facultad de Ingenier铆a - Computaci贸n</h3>\n",
    "      <p style=\"font-size: 16px; margin-bottom: 0; margin-top: -20px\">\n",
    "        <strong>Curso:</strong> CC3104 - Aprendizaje por Refuerzo \n",
    "        <strong>Secci贸n:</strong> 10\n",
    "      </p>\n",
    "      <p style=\"font-size: 16px; margin: 0;\"><strong>Laboratorio 4:</strong> M茅todos de Monte Carlo</p>\n",
    "      <br>\n",
    "      <p style=\"font-size: 15px; margin: 0;\"><strong>Autores:</strong></p>\n",
    "      <ul style=\"margin-top: 5px; padding-left: 20px; font-size: 15px;\">\n",
    "        <li>Diego Alexander Hern谩ndez Silvestre - <strong>21270</strong></li>\n",
    "        <li>Linda In茅s Jim茅nez Vides - <strong>21169</strong></li>\n",
    "        <li>Mario Antonio Guerra Morales - <strong>21008</strong></li>\n",
    "      </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e0934d",
   "metadata": {},
   "source": [
    "##  Task 1\n",
    "\n",
    "**1. 驴C贸mo afecta la elecci贸n de la estrategia de exploraci贸n (exploring starts vs soft policy) a la precisi贸n de la evaluaci贸n de pol铆ticas en los m茅todos de Monte Carlo? Considere la posibilidad de comparar el desempe帽o de las pol铆ticas evaluadas con y sin explorar los inicios o con diferentes niveles de exploraci贸n en pol铆ticas blandas.**\n",
    "\n",
    "- La elecci贸n de estrategia de exploraci贸n s铆 tiene un impacto directo en la evaluaci贸n de pol铆ticas al usar los m茅todos de Monte Carlo. Si se utiliza Exploring Starts, se garantiza una mayor diversidad en las trayectorias debido a que se puede iniciar desde distintas combinaciones de estados y de acciones. Mientras que, con Soft Policies se a帽ade un equilibrio entre la exploraci贸n y la explotaci贸n. Cuando se mantiene una probabilidad de selecci贸n de im谩genes aleatorias, se a帽ade la posibilidad de seguir explorando nuevas y aleatorias trayectorias, a煤n priorizando acciones de mayor valor.\n",
    "\n",
    "**2. En el contexto del aprendizaje de Monte Carlo fuera de la p贸liza, 驴c贸mo afecta la raz贸n de muestreo de importancia a la convergencia de la evaluaci贸n de pol铆ticas? Explore c贸mo la raz贸n de muestreo de importancia afecta la estabilidad y la convergencia.**\n",
    "\n",
    "- La raz贸n de muestreo de importancia ajusta las estimaciones de retorno para que la pol铆tica objetivo se refleje utilizando datos de manera diferente. Esta afecta al permitirse aprender sobre una pol铆tica objetivo sin hacer un seguimiento directo, pero tambi茅n es capaz de hacer inestables las razones si tanto el comportamiento como el objetivo difieren demasiado.\n",
    "\n",
    "**3. 驴C贸mo puede el uso de una soft policy influir en la eficacia del aprendizaje de pol铆ticas 贸ptimas en comparaci贸n con las pol铆ticas deterministas en los m茅todos de Monte Carlo? Compare el desempe帽o y los resultados de aprendizaje de las pol铆ticas derivadas de estrategias 茅psilon-greedy con las derivadas de pol铆ticas deterministas.**\n",
    "\n",
    "- Una soft policy puede influir con una mayor exploraci贸n, esto para ayudar al descubrimiento de acciones que no son efectivas a corto plazo pero s铆 a un largo plazo. Mientras que, una pol铆tica determinista elegir谩 la misma acci贸n siempre, lo que puede llegar a darse el riesgo de no hacer una correcta exploraci贸n.\n",
    "\n",
    "**4. 驴Cu谩les son los posibles beneficios y desventajas de utilizar m茅todos de Monte Carlo off-policy en comparaci贸n con los on-policy en t茅rminos de eficiencia de la muestra, costo computacional y velocidad de aprendizaje?**\n",
    "\n",
    "- Los m茅todos de Monte Carlo off-policy ofrecen una mejor eficiencia al existir la posibilidad de usar datos previos o simulados, pero ello contempla un mayor costo computacional. Sin embargo, esto a su vez permite una gran flexibilidad y con ello una velocidad de aprendizaje considerable, si es que las pol铆ticas no difieren mucho con el objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1376915",
   "metadata": {},
   "source": [
    "##  Task 2\n",
    "\n",
    "En este ejercicio, simular谩 un sistema de gesti贸n de inventarios para una peque帽a tienda minorista. La tienda tiene como objetivo maximizar las ganancias manteniendo niveles 贸ptimos de existencias de diferentes productos. Utilizar谩 m茅todos de Monte Carlo para la evaluaci贸n de p贸lizas, exploring starts, soft policies y aprendizaje off-policy para estimar el valor de diferentes estrategias de gesti贸n de inventarios. Su objetivo es implementar una soluci贸n en Python y responder preguntas espec铆ficas en funci贸n de los resultados.\n",
    "\n",
    "**Definici贸n del entorno**\n",
    "- Utilice el ambiente dado m谩s adelante para simular el entorno de la tienda. Considere que:\n",
    "    - El estado representa los niveles de existencias actuales de los productos.\n",
    "    - Las acciones representan decisiones sobre cu谩nto reponer de cada producto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "396c0503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class InventoryEnvironment:\n",
    "    def __init__(self):\n",
    "        self.products = ['product_A', 'product_B']\n",
    "        self.max_stock = 10\n",
    "        self.demand = {'product_A': [0, 1, 2], 'product_B': [0, 1, 2]}\n",
    "        self.restock_cost = {'product_A': 5, 'product_B': 7}\n",
    "        self.sell_price = {'product_A': 10, 'product_B': 15}\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = {product: random.randint(0, self.max_stock) for product in self.products}\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        for product in self.products:\n",
    "            stock = self.state[product]\n",
    "            restock = action[product]\n",
    "            self.state[product] = min(self.max_stock, stock + restock)\n",
    "            demand = random.choice(self.demand[product])\n",
    "            sales = min(demand, self.state[product])\n",
    "            self.state[product] -= sales\n",
    "            reward += sales * self.sell_price[product] - restock * self.restock_cost[product]\n",
    "        return self.state.copy(), reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034ae583",
   "metadata": {},
   "source": [
    "**Generaci贸n de episodios**\n",
    "- Cada episodio representa una serie de d铆as en los que la tienda sigue una pol铆tica de inventario espec铆fica.\n",
    "- Debe recopilar datos para varios episodios y registrar las recompensas (ganancias) de cada d铆a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c17973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, policy, num_days=10):\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "\n",
    "    for _ in range(num_days):\n",
    "        # Elegir acci贸n seg煤n la pol铆tica\n",
    "        action = policy(state)\n",
    "        # Ejecutar acci贸n en el entorno\n",
    "        next_state, reward = env.step(action)\n",
    "        # Guardar transici贸n: estado, acci贸n, recompensa\n",
    "        episode.append((state.copy(), action.copy(), reward))\n",
    "        # Actualizar estado actual\n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9281d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    return {\n",
    "        'product_A': random.randint(0, 3),\n",
    "        'product_B': random.randint(0, 3)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b45f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episodes(env, policy, num_episodes=100, num_days=10):\n",
    "    episodes = []\n",
    "    for _ in range(num_episodes):\n",
    "        episode = generate_episode(env, policy, num_days)\n",
    "        episodes.append(episode)\n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2fe8237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Episodio 1 ---\n",
      "D铆a 1: Estado={'product_A': 2, 'product_B': 2}, Acci贸n={'product_A': 1, 'product_B': 0}, Recompensa=15\n",
      "D铆a 2: Estado={'product_A': 2, 'product_B': 2}, Acci贸n={'product_A': 0, 'product_B': 1}, Recompensa=43\n",
      "D铆a 3: Estado={'product_A': 0, 'product_B': 1}, Acci贸n={'product_A': 2, 'product_B': 2}, Recompensa=26\n",
      "D铆a 4: Estado={'product_A': 0, 'product_B': 1}, Acci贸n={'product_A': 3, 'product_B': 3}, Recompensa=14\n",
      "D铆a 5: Estado={'product_A': 1, 'product_B': 2}, Acci贸n={'product_A': 0, 'product_B': 0}, Recompensa=25\n",
      "\n",
      "--- Episodio 2 ---\n",
      "D铆a 1: Estado={'product_A': 2, 'product_B': 10}, Acci贸n={'product_A': 2, 'product_B': 3}, Recompensa=-31\n",
      "D铆a 2: Estado={'product_A': 2, 'product_B': 10}, Acci贸n={'product_A': 3, 'product_B': 1}, Recompensa=3\n",
      "D铆a 3: Estado={'product_A': 4, 'product_B': 9}, Acci贸n={'product_A': 1, 'product_B': 3}, Recompensa=-6\n",
      "D铆a 4: Estado={'product_A': 3, 'product_B': 10}, Acci贸n={'product_A': 0, 'product_B': 1}, Recompensa=43\n",
      "D铆a 5: Estado={'product_A': 1, 'product_B': 8}, Acci贸n={'product_A': 0, 'product_B': 3}, Recompensa=-11\n",
      "\n",
      "--- Episodio 3 ---\n",
      "D铆a 1: Estado={'product_A': 0, 'product_B': 9}, Acci贸n={'product_A': 0, 'product_B': 3}, Recompensa=-6\n",
      "D铆a 2: Estado={'product_A': 0, 'product_B': 9}, Acci贸n={'product_A': 3, 'product_B': 3}, Recompensa=14\n",
      "D铆a 3: Estado={'product_A': 1, 'product_B': 8}, Acci贸n={'product_A': 2, 'product_B': 0}, Recompensa=5\n",
      "D铆a 4: Estado={'product_A': 3, 'product_B': 7}, Acci贸n={'product_A': 1, 'product_B': 1}, Recompensa=8\n",
      "D铆a 5: Estado={'product_A': 2, 'product_B': 8}, Acci贸n={'product_A': 1, 'product_B': 0}, Recompensa=45\n"
     ]
    }
   ],
   "source": [
    "env = InventoryEnvironment()\n",
    "episodes = generate_episodes(env, random_policy, num_episodes=3, num_days=5)\n",
    "\n",
    "for i, ep in enumerate(episodes):\n",
    "    print(f\"\\n--- Episodio {i+1} ---\")\n",
    "    for day, (state, action, reward) in enumerate(ep):\n",
    "        print(f\"D铆a {day+1}: Estado={state}, Acci贸n={action}, Recompensa={reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be23d661",
   "metadata": {},
   "source": [
    "**Exploring Starts**\n",
    "- Implemente explorar inicios para garantizar un conjunto diverso de estados y acciones iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd9d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_exploring_starts(env, policy, num_days=10):\n",
    "    # Estado inicial aleatorio\n",
    "    state = {\n",
    "        product: random.randint(0, env.max_stock)\n",
    "        for product in env.products\n",
    "    }\n",
    "    env.state = state.copy()\n",
    "\n",
    "    episode = []\n",
    "\n",
    "    for day in range(num_days):\n",
    "        # Acci贸n inicial aleatoria solo en el primer paso\n",
    "        if day == 0:\n",
    "            action = {\n",
    "                product: random.randint(0, 3)\n",
    "                for product in env.products\n",
    "            }\n",
    "        else:\n",
    "            action = policy(state)\n",
    "\n",
    "        next_state, reward = env.step(action)\n",
    "        episode.append((state.copy(), action.copy(), reward))\n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f1c72a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episodes_exploring_starts(env, policy, num_episodes=5, num_days=7):\n",
    "    episodes = []\n",
    "    for _ in range(num_episodes):\n",
    "        episode = generate_episode_exploring_starts(env, policy, num_days)\n",
    "        episodes.append(episode)\n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55d95aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Episodio (Exploring Starts) 1 ---\n",
      "D铆a 1: Estado={'product_A': 1, 'product_B': 0}, Acci贸n={'product_A': 3, 'product_B': 3}, Recompensa=4\n",
      "D铆a 2: Estado={'product_A': 3, 'product_B': 1}, Acci贸n={'product_A': 2, 'product_B': 1}, Recompensa=18\n",
      "D铆a 3: Estado={'product_A': 3, 'product_B': 1}, Acci贸n={'product_A': 2, 'product_B': 1}, Recompensa=13\n",
      "D铆a 4: Estado={'product_A': 5, 'product_B': 0}, Acci贸n={'product_A': 1, 'product_B': 1}, Recompensa=3\n",
      "D铆a 5: Estado={'product_A': 6, 'product_B': 0}, Acci贸n={'product_A': 0, 'product_B': 0}, Recompensa=20\n",
      "\n",
      "--- Episodio (Exploring Starts) 2 ---\n",
      "D铆a 1: Estado={'product_A': 2, 'product_B': 6}, Acci贸n={'product_A': 3, 'product_B': 1}, Recompensa=-2\n",
      "D铆a 2: Estado={'product_A': 3, 'product_B': 7}, Acci贸n={'product_A': 0, 'product_B': 2}, Recompensa=36\n",
      "D铆a 3: Estado={'product_A': 1, 'product_B': 7}, Acci贸n={'product_A': 2, 'product_B': 1}, Recompensa=33\n",
      "D铆a 4: Estado={'product_A': 1, 'product_B': 6}, Acci贸n={'product_A': 1, 'product_B': 0}, Recompensa=10\n",
      "D铆a 5: Estado={'product_A': 2, 'product_B': 5}, Acci贸n={'product_A': 3, 'product_B': 2}, Recompensa=-14\n",
      "\n",
      "--- Episodio (Exploring Starts) 3 ---\n",
      "D铆a 1: Estado={'product_A': 9, 'product_B': 1}, Acci贸n={'product_A': 1, 'product_B': 3}, Recompensa=-6\n",
      "D铆a 2: Estado={'product_A': 8, 'product_B': 4}, Acci贸n={'product_A': 0, 'product_B': 0}, Recompensa=35\n",
      "D铆a 3: Estado={'product_A': 6, 'product_B': 3}, Acci贸n={'product_A': 3, 'product_B': 3}, Recompensa=-36\n",
      "D铆a 4: Estado={'product_A': 9, 'product_B': 6}, Acci贸n={'product_A': 0, 'product_B': 2}, Recompensa=21\n",
      "D铆a 5: Estado={'product_A': 7, 'product_B': 7}, Acci贸n={'product_A': 3, 'product_B': 3}, Recompensa=4\n"
     ]
    }
   ],
   "source": [
    "episodes_es = generate_episodes_exploring_starts(env, random_policy, num_episodes=3, num_days=5)\n",
    "\n",
    "for i, ep in enumerate(episodes_es):\n",
    "    print(f\"\\n--- Episodio (Exploring Starts) {i+1} ---\")\n",
    "    for day, (state, action, reward) in enumerate(ep):\n",
    "        print(f\"D铆a {day+1}: Estado={state}, Acci贸n={action}, Recompensa={reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d3fed",
   "metadata": {},
   "source": [
    "**Soft Policies**\n",
    "- Utilice una soft policy (como epsilon-greedy) para garantizar un equilibrio entre la exploraci贸n y la\n",
    "explotaci贸n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5868024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Episodio (Soft Policy) 1 ---\n",
      "D铆a 1: Estado={'product_A': 2, 'product_B': 2}, Acci贸n={'product_A': 1, 'product_B': 0}, Recompensa=15\n",
      "D铆a 2: Estado={'product_A': 2, 'product_B': 2}, Acci贸n={'product_A': 0, 'product_B': 1}, Recompensa=43\n",
      "D铆a 3: Estado={'product_A': 0, 'product_B': 1}, Acci贸n={'product_A': 2, 'product_B': 2}, Recompensa=26\n",
      "D铆a 4: Estado={'product_A': 0, 'product_B': 1}, Acci贸n={'product_A': 3, 'product_B': 3}, Recompensa=14\n",
      "D铆a 5: Estado={'product_A': 1, 'product_B': 2}, Acci贸n={'product_A': 0, 'product_B': 0}, Recompensa=25\n",
      "\n",
      "--- Episodio (Soft Policy) 2 ---\n",
      "D铆a 1: Estado={'product_A': 2, 'product_B': 10}, Acci贸n={'product_A': 2, 'product_B': 3}, Recompensa=-31\n",
      "D铆a 2: Estado={'product_A': 2, 'product_B': 10}, Acci贸n={'product_A': 3, 'product_B': 1}, Recompensa=3\n",
      "D铆a 3: Estado={'product_A': 4, 'product_B': 9}, Acci贸n={'product_A': 1, 'product_B': 3}, Recompensa=-6\n",
      "D铆a 4: Estado={'product_A': 3, 'product_B': 10}, Acci贸n={'product_A': 0, 'product_B': 1}, Recompensa=43\n",
      "D铆a 5: Estado={'product_A': 1, 'product_B': 8}, Acci贸n={'product_A': 0, 'product_B': 3}, Recompensa=-11\n",
      "\n",
      "--- Episodio (Soft Policy) 3 ---\n",
      "D铆a 1: Estado={'product_A': 0, 'product_B': 9}, Acci贸n={'product_A': 0, 'product_B': 3}, Recompensa=-6\n",
      "D铆a 2: Estado={'product_A': 0, 'product_B': 9}, Acci贸n={'product_A': 3, 'product_B': 3}, Recompensa=14\n",
      "D铆a 3: Estado={'product_A': 1, 'product_B': 8}, Acci贸n={'product_A': 2, 'product_B': 0}, Recompensa=5\n",
      "D铆a 4: Estado={'product_A': 3, 'product_B': 7}, Acci贸n={'product_A': 1, 'product_B': 1}, Recompensa=8\n",
      "D铆a 5: Estado={'product_A': 2, 'product_B': 8}, Acci贸n={'product_A': 1, 'product_B': 0}, Recompensa=45\n",
      "\n",
      "--- Episodio (Soft Policy) 4 ---\n",
      "D铆a 1: Estado={'product_A': 10, 'product_B': 1}, Acci贸n={'product_A': 3, 'product_B': 0}, Recompensa=0\n",
      "D铆a 2: Estado={'product_A': 10, 'product_B': 1}, Acci贸n={'product_A': 3, 'product_B': 1}, Recompensa=28\n",
      "D铆a 3: Estado={'product_A': 8, 'product_B': 0}, Acci贸n={'product_A': 1, 'product_B': 3}, Recompensa=-16\n",
      "D铆a 4: Estado={'product_A': 8, 'product_B': 3}, Acci贸n={'product_A': 2, 'product_B': 3}, Recompensa=-1\n",
      "D铆a 5: Estado={'product_A': 10, 'product_B': 4}, Acci贸n={'product_A': 0, 'product_B': 0}, Recompensa=10\n",
      "\n",
      "--- Episodio (Soft Policy) 5 ---\n",
      "D铆a 1: Estado={'product_A': 2, 'product_B': 2}, Acci贸n={'product_A': 1, 'product_B': 2}, Recompensa=16\n",
      "D铆a 2: Estado={'product_A': 2, 'product_B': 2}, Acci贸n={'product_A': 0, 'product_B': 1}, Recompensa=23\n",
      "D铆a 3: Estado={'product_A': 2, 'product_B': 1}, Acci贸n={'product_A': 2, 'product_B': 3}, Recompensa=-16\n",
      "D铆a 4: Estado={'product_A': 4, 'product_B': 3}, Acci贸n={'product_A': 0, 'product_B': 0}, Recompensa=20\n",
      "D铆a 5: Estado={'product_A': 2, 'product_B': 3}, Acci贸n={'product_A': 0, 'product_B': 3}, Recompensa=9\n",
      "\n",
      "--- Episodio (Soft Policy) 6 ---\n",
      "D铆a 1: Estado={'product_A': 4, 'product_B': 5}, Acci贸n={'product_A': 3, 'product_B': 3}, Recompensa=-36\n",
      "D铆a 2: Estado={'product_A': 4, 'product_B': 5}, Acci贸n={'product_A': 2, 'product_B': 0}, Recompensa=0\n",
      "D铆a 3: Estado={'product_A': 5, 'product_B': 5}, Acci贸n={'product_A': 2, 'product_B': 1}, Recompensa=3\n",
      "D铆a 4: Estado={'product_A': 5, 'product_B': 6}, Acci贸n={'product_A': 3, 'product_B': 1}, Recompensa=3\n",
      "D铆a 5: Estado={'product_A': 7, 'product_B': 6}, Acci贸n={'product_A': 1, 'product_B': 2}, Recompensa=21\n",
      "\n",
      "--- Episodio (Soft Policy) 7 ---\n",
      "D铆a 1: Estado={'product_A': 8, 'product_B': 9}, Acci贸n={'product_A': 1, 'product_B': 3}, Recompensa=-1\n",
      "D铆a 2: Estado={'product_A': 8, 'product_B': 9}, Acci贸n={'product_A': 0, 'product_B': 1}, Recompensa=28\n",
      "D铆a 3: Estado={'product_A': 6, 'product_B': 9}, Acci贸n={'product_A': 3, 'product_B': 3}, Recompensa=-6\n",
      "D铆a 4: Estado={'product_A': 9, 'product_B': 8}, Acci贸n={'product_A': 1, 'product_B': 2}, Recompensa=1\n",
      "D铆a 5: Estado={'product_A': 8, 'product_B': 10}, Acci贸n={'product_A': 0, 'product_B': 2}, Recompensa=-4\n",
      "\n",
      "--- Episodio (Soft Policy) 8 ---\n",
      "D铆a 1: Estado={'product_A': 6, 'product_B': 9}, Acci贸n={'product_A': 3, 'product_B': 2}, Recompensa=-14\n",
      "D铆a 2: Estado={'product_A': 6, 'product_B': 9}, Acci贸n={'product_A': 1, 'product_B': 3}, Recompensa=-11\n",
      "D铆a 3: Estado={'product_A': 7, 'product_B': 9}, Acci贸n={'product_A': 2, 'product_B': 2}, Recompensa=-14\n",
      "D铆a 4: Estado={'product_A': 8, 'product_B': 10}, Acci贸n={'product_A': 3, 'product_B': 0}, Recompensa=15\n",
      "D铆a 5: Estado={'product_A': 10, 'product_B': 8}, Acci贸n={'product_A': 3, 'product_B': 1}, Recompensa=3\n"
     ]
    }
   ],
   "source": [
    "Q = {} # Q-table para almacenar valores de acci贸n-estado = [list of returns]\n",
    "\n",
    "def epsilon_greedy_policy(Q, state, epsilon=0.1):\n",
    "    state_key = str(state)\n",
    "    if state_key not in Q or random.random() < epsilon:\n",
    "        # Seleccionar acci贸n aleatoria\n",
    "        return {\n",
    "            'product_A': random.randint(0, 3),\n",
    "            'product_B': random.randint(0, 3)\n",
    "        }\n",
    "    else:\n",
    "        # Seleccionar la mejor acci贸n basada en Q\n",
    "        best_action = max(Q[state_key], key=lambda a: np.mean(Q[state_key][a]))\n",
    "        return eval(best_action)\n",
    "\n",
    "def generate_episodes_soft_policy(env, Q, num_episodes=5, num_days=7, epsilon=0.1):\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode = []\n",
    "\n",
    "        for _ in range(num_days):\n",
    "            action = epsilon_greedy_policy(Q, state, epsilon)\n",
    "            next_state, reward = env.step(action)\n",
    "            episode.append((state.copy(), action.copy(), reward))\n",
    "            state = next_state\n",
    "\n",
    "        episodes.append(episode)\n",
    "    return episodes\n",
    "\n",
    "def update_Q(Q, episodes):\n",
    "    for episode in episodes:\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for state, action, reward in reversed(episode):\n",
    "            G += reward\n",
    "            state_key = str(state)\n",
    "            action_key = str(action)\n",
    "\n",
    "            if (state_key, action_key) not in visited:\n",
    "                visited.add((state_key, action_key))\n",
    "                if state_key not in Q:\n",
    "                    Q[state_key] = {}\n",
    "                if action_key not in Q[state_key]:\n",
    "                    Q[state_key][action_key] = []\n",
    "                Q[state_key][action_key].append(G)\n",
    "\n",
    "\n",
    "Q = {}\n",
    "episodes_soft = generate_episodes_soft_policy(env, Q, epsilon=0.1, num_episodes=5, num_days=5)\n",
    "update_Q(Q, episodes_soft)\n",
    "\n",
    "# Ver resultados\n",
    "for i, ep in enumerate(episodes_soft):\n",
    "    print(f\"\\n--- Episodio (Soft Policy) {i+1} ---\")\n",
    "    for day, (state, action, reward) in enumerate(ep):\n",
    "        print(f\"D铆a {day+1}: Estado={state}, Acci贸n={action}, Recompensa={reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e209b7",
   "metadata": {},
   "source": [
    "**Aprendizaje off-policy**\n",
    "- Implemente el aprendizaje off-policy para evaluar una pol铆tica objetivo utilizando datos generados\n",
    "por una pol铆tica de comportamiento diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32838af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f86034b",
   "metadata": {},
   "source": [
    "1. 驴Cu谩l es el valor estimado de mantener diferentes niveles de existencias para cada producto?\n",
    "\n",
    "2. 驴C贸mo afecta el valor epsilon en la pol铆tica blanda al rendimiento?\n",
    "\n",
    "3. 驴Cu谩l es el impacto de utilizar el aprendizaje fuera de la pol铆tica en comparaci贸n con el aprendizaje dentro de la pol铆tica?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
