{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c5ba4e",
   "metadata": {},
   "source": [
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <td style=\"width:20%; vertical-align:middle;\">\n",
    "      <img src=\"LogoUVG.png\" width=\"400\"/>\n",
    "    </td>\n",
    "    <td style=\"text-align:left; vertical-align:middle;\">\n",
    "      <h2 style=\"margin-bottom: 0;\">Universidad del Valle de Guatemala - UVG</h2>\n",
    "      <h3 style=\"margin-top: 0;\">Facultad de Ingenier铆a - Computaci贸n</h3>\n",
    "      <p style=\"font-size: 16px; margin-bottom: 0; margin-top: -20px\">\n",
    "        <strong>Curso:</strong> CC3104 - Aprendizaje por Refuerzo \n",
    "        <strong>Secci贸n:</strong> 10\n",
    "      </p>\n",
    "      <p style=\"font-size: 16px; margin: 0;\"><strong>Laboratorio 4:</strong> M茅todos de Monte Carlo</p>\n",
    "      <br>\n",
    "      <p style=\"font-size: 15px; margin: 0;\"><strong>Autores:</strong></p>\n",
    "      <ul style=\"margin-top: 5px; padding-left: 20px; font-size: 15px;\">\n",
    "        <li>Diego Alexander Hern谩ndez Silvestre - <strong>21270</strong></li>\n",
    "        <li>Linda In茅s Jim茅nez Vides - <strong>21169</strong></li>\n",
    "        <li>Mario Antonio Guerra Morales - <strong>21008</strong></li>\n",
    "      </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e0934d",
   "metadata": {},
   "source": [
    "##  Task 1\n",
    "\n",
    "**1. 驴C贸mo afecta la elecci贸n de la estrategia de exploraci贸n (exploring starts vs soft policy) a la precisi贸n de la evaluaci贸n de pol铆ticas en los m茅todos de Monte Carlo? Considere la posibilidad de comparar el desempe帽o de las pol铆ticas evaluadas con y sin explorar los inicios o con diferentes niveles de exploraci贸n en pol铆ticas blandas.**\n",
    "\n",
    "- La elecci贸n de estrategia de exploraci贸n s铆 tiene un impacto directo en la evaluaci贸n de pol铆ticas al usar los m茅todos de Monte Carlo. Si se utiliza Exploring Starts, se garantiza una mayor diversidad en las trayectorias debido a que se puede iniciar desde distintas combinaciones de estados y de acciones. Mientras que, con Soft Policies se a帽ade un equilibrio entre la exploraci贸n y la explotaci贸n. Cuando se mantiene una probabilidad de selecci贸n de im谩genes aleatorias, se a帽ade la posibilidad de seguir explorando nuevas y aleatorias trayectorias, a煤n priorizando acciones de mayor valor.\n",
    "\n",
    "**2. En el contexto del aprendizaje de Monte Carlo fuera de la p贸liza, 驴c贸mo afecta la raz贸n de muestreo de importancia a la convergencia de la evaluaci贸n de pol铆ticas? Explore c贸mo la raz贸n de muestreo de importancia afecta la estabilidad y la convergencia.**\n",
    "\n",
    "- La raz贸n de muestreo de importancia ajusta las estimaciones de retorno para que la pol铆tica objetivo se refleje utilizando datos de manera diferente. Esta afecta al permitirse aprender sobre una pol铆tica objetivo sin hacer un seguimiento directo, pero tambi茅n es capaz de hacer inestables las razones si tanto el comportamiento como el objetivo difieren demasiado.\n",
    "\n",
    "**3. 驴C贸mo puede el uso de una soft policy influir en la eficacia del aprendizaje de pol铆ticas 贸ptimas en comparaci贸n con las pol铆ticas deterministas en los m茅todos de Monte Carlo? Compare el desempe帽o y los resultados de aprendizaje de las pol铆ticas derivadas de estrategias 茅psilon-greedy con las derivadas de pol铆ticas deterministas.**\n",
    "\n",
    "- Una soft policy puede influir con una mayor exploraci贸n, esto para ayudar al descubrimiento de acciones que no son efectivas a corto plazo pero s铆 a un largo plazo. Mientras que, una pol铆tica determinista elegir谩 la misma acci贸n siempre, lo que puede llegar a darse el riesgo de no hacer una correcta exploraci贸n.\n",
    "\n",
    "**4. 驴Cu谩les son los posibles beneficios y desventajas de utilizar m茅todos de Monte Carlo off-policy en comparaci贸n con los on-policy en t茅rminos de eficiencia de la muestra, costo computacional y velocidad de aprendizaje?**\n",
    "\n",
    "- Los m茅todos de Monte Carlo off-policy ofrecen una mejor eficiencia al existir la posibilidad de usar datos previos o simulados, pero ello contempla un mayor costo computacional. Sin embargo, esto a su vez permite una gran flexibilidad y con ello una velocidad de aprendizaje considerable, si es que las pol铆ticas no difieren mucho con el objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1376915",
   "metadata": {},
   "source": [
    "##  Task 2\n",
    "\n",
    "En este ejercicio, simular谩 un sistema de gesti贸n de inventarios para una peque帽a tienda minorista. La tienda tiene como objetivo maximizar las ganancias manteniendo niveles 贸ptimos de existencias de diferentes productos. Utilizar谩 m茅todos de Monte Carlo para la evaluaci贸n de p贸lizas, exploring starts, soft policies y aprendizaje off-policy para estimar el valor de diferentes estrategias de gesti贸n de inventarios. Su objetivo es implementar una soluci贸n en Python y responder preguntas espec铆ficas en funci贸n de los resultados.\n",
    "\n",
    "**Definici贸n del entorno**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c0503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class InventoryEnvironment:\n",
    "    def __init__(self):\n",
    "        self.products = ['product_A', 'product_B']\n",
    "        self.max_stock = 10\n",
    "        self.demand = {'product_A': [0, 1, 2], 'product_B': [0, 1, 2]}\n",
    "        self.restock_cost = {'product_A': 5, 'product_B': 7}\n",
    "        self.sell_price = {'product_A': 10, 'product_B': 15}\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = {product: random.randint(0, self.max_stock) for product in self.products}\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        for product in self.products:\n",
    "            stock = self.state[product]\n",
    "            restock = action[product]\n",
    "            self.state[product] = min(self.max_stock, stock + restock)\n",
    "            demand = random.choice(self.demand[product])\n",
    "            sales = min(demand, self.state[product])\n",
    "            self.state[product] -= sales\n",
    "            reward += sales * self.sell_price[product] - restock * self.restock_cost[product]\n",
    "        return self.state.copy(), reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034ae583",
   "metadata": {},
   "source": [
    "**Generaci贸n de episodios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9281d98a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be23d661",
   "metadata": {},
   "source": [
    "**Exploring Starts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9d198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c9d3fed",
   "metadata": {},
   "source": [
    "**Soft Policies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5868024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68e209b7",
   "metadata": {},
   "source": [
    "**Aprendizaje off-policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32838af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f86034b",
   "metadata": {},
   "source": [
    "1. 驴Cu谩l es el valor estimado de mantener diferentes niveles de existencias para cada producto?\n",
    "\n",
    "2. 驴C贸mo afecta el valor epsilon en la pol铆tica blanda al rendimiento?\n",
    "\n",
    "3. 驴Cu谩l es el impacto de utilizar el aprendizaje fuera de la pol铆tica en comparaci贸n con el aprendizaje dentro de la pol铆tica?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
