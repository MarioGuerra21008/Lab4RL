{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c5ba4e",
   "metadata": {},
   "source": [
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <td style=\"width:20%; vertical-align:middle;\">\n",
    "      <img src=\"LogoUVG.png\" width=\"400\"/>\n",
    "    </td>\n",
    "    <td style=\"text-align:left; vertical-align:middle;\">\n",
    "      <h2 style=\"margin-bottom: 0;\">Universidad del Valle de Guatemala - UVG</h2>\n",
    "      <h3 style=\"margin-top: 0;\">Facultad de Ingeniería - Computación</h3>\n",
    "      <p style=\"font-size: 16px; margin-bottom: 0; margin-top: -20px\">\n",
    "        <strong>Curso:</strong> CC3104 - Aprendizaje por Refuerzo \n",
    "        <strong>Sección:</strong> 10\n",
    "      </p>\n",
    "      <p style=\"font-size: 16px; margin: 0;\"><strong>Laboratorio 4:</strong> Métodos de Monte Carlo</p>\n",
    "      <br>\n",
    "      <p style=\"font-size: 15px; margin: 0;\"><strong>Autores:</strong></p>\n",
    "      <ul style=\"margin-top: 5px; padding-left: 20px; font-size: 15px;\">\n",
    "        <li>Diego Alexander Hernández Silvestre - <strong>21270</strong></li>\n",
    "        <li>Linda Inés Jiménez Vides - <strong>21169</strong></li>\n",
    "        <li>Mario Antonio Guerra Morales - <strong>21008</strong></li>\n",
    "      </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e0934d",
   "metadata": {},
   "source": [
    "## 📝 Task 1\n",
    "\n",
    "**1. ¿Cómo afecta la elección de la estrategia de exploración (exploring starts vs soft policy) a la precisión de la evaluación de políticas en los métodos de Monte Carlo? Considere la posibilidad de comparar el desempeño de las políticas evaluadas con y sin explorar los inicios o con diferentes niveles de exploración en políticas blandas.**\n",
    "\n",
    "- La elección de estrategia de exploración sí tiene un impacto directo en la evaluación de políticas al usar los métodos de Monte Carlo. Si se utiliza Exploring Starts, se garantiza una mayor diversidad en las trayectorias debido a que se puede iniciar desde distintas combinaciones de estados y de acciones. Mientras que, con Soft Policies se añade un equilibrio entre la exploración y la explotación. Cuando se mantiene una probabilidad de selección de acciones aleatorias, se añade la posibilidad de seguir explorando nuevas y aleatorias trayectorias, aún priorizando acciones de mayor valor.\n",
    "\n",
    "**2. En el contexto del aprendizaje de Monte Carlo fuera de la póliza, ¿cómo afecta la razón de muestreo de importancia a la convergencia de la evaluación de políticas? Explore cómo la razón de muestreo de importancia afecta la estabilidad y la convergencia.**\n",
    "\n",
    "- La razón de muestreo de importancia ajusta las estimaciones de retorno para que la política objetivo se refleje utilizando datos de manera diferente. Esta afecta al permitirse aprender sobre una política objetivo sin hacer un seguimiento directo, pero también es capaz de hacer inestables las razones si tanto el comportamiento como el objetivo difieren demasiado.\n",
    "\n",
    "**3. ¿Cómo puede el uso de una soft policy influir en la eficacia del aprendizaje de políticas óptimas en comparación con las políticas deterministas en los métodos de Monte Carlo? Compare el desempeño y los resultados de aprendizaje de las políticas derivadas de estrategias épsilon-greedy con las derivadas de políticas deterministas.**\n",
    "\n",
    "- Una soft policy puede influir con una mayor exploración, esto para ayudar al descubrimiento de acciones que no son efectivas a corto plazo pero sí a un largo plazo. Mientras que, una política determinista elegirá la misma acción siempre, lo que puede llegar a darse el riesgo de no hacer una correcta exploración.\n",
    "\n",
    "**4. ¿Cuáles son los posibles beneficios y desventajas de utilizar métodos de Monte Carlo off-policy en comparación con los on-policy en términos de eficiencia de la muestra, costo computacional y velocidad de aprendizaje?**\n",
    "\n",
    "- Los métodos de Monte Carlo off-policy ofrecen una mejor eficiencia al existir la posibilidad de usar datos previos o simulados, pero ello contempla un mayor costo computacional. Sin embargo, esto a su vez permite una gran flexibilidad y con ello una velocidad de aprendizaje considerable, si es que las políticas no difieren mucho con el objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1376915",
   "metadata": {},
   "source": [
    "## 📝 Task 2\n",
    "\n",
    "En este ejercicio, simulará un sistema de gestión de inventarios para una pequeña tienda minorista. La tienda tiene como objetivo maximizar las ganancias manteniendo niveles óptimos de existencias de diferentes productos. Utilizará métodos de Monte Carlo para la evaluación de pólizas, exploring starts, soft policies y aprendizaje off-policy para estimar el valor de diferentes estrategias de gestión de inventarios. Su objetivo es implementar una solución en Python y responder preguntas específicas en función de los resultados.\n",
    "\n",
    "**Definición del entorno**\n",
    "- Utilice el ambiente dado más adelante para simular el entorno de la tienda. Considere que:\n",
    "    - El estado representa los niveles de existencias actuales de los productos.\n",
    "    - Las acciones representan decisiones sobre cuánto reponer de cada producto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "396c0503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class InventoryEnvironment:\n",
    "    def __init__(self):\n",
    "        self.products = ['product_A', 'product_B']\n",
    "        self.max_stock = 10\n",
    "        self.demand = {'product_A': [0, 1, 2], 'product_B': [0, 1, 2]}\n",
    "        self.restock_cost = {'product_A': 5, 'product_B': 7}\n",
    "        self.sell_price = {'product_A': 10, 'product_B': 15}\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = {product: random.randint(0, self.max_stock) for product in self.products}\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        for product in self.products:\n",
    "            stock = self.state[product]\n",
    "            restock = action[product]\n",
    "            self.state[product] = min(self.max_stock, stock + restock)\n",
    "            demand = random.choice(self.demand[product])\n",
    "            sales = min(demand, self.state[product])\n",
    "            self.state[product] -= sales\n",
    "            reward += sales * self.sell_price[product] - restock * self.restock_cost[product]\n",
    "        return self.state.copy(), reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034ae583",
   "metadata": {},
   "source": [
    "**Generación de episodios**\n",
    "- Cada episodio representa una serie de días en los que la tienda sigue una política de inventario específica.\n",
    "- Debe recopilar datos para varios episodios y registrar las recompensas (ganancias) de cada día"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "99c17973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, policy, num_days=10):\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "\n",
    "    for _ in range(num_days):\n",
    "        # Elegir acción según la política\n",
    "        action = policy(state)\n",
    "        # Ejecutar acción en el entorno\n",
    "        next_state, reward = env.step(action)\n",
    "        # Guardar transición: estado, acción, recompensa\n",
    "        episode.append((state.copy(), action.copy(), reward))\n",
    "        # Actualizar estado actual\n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9281d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    return {\n",
    "        'product_A': random.randint(0, 3),\n",
    "        'product_B': random.randint(0, 3)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4b45f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episodes(env, policy, num_episodes=100, num_days=10):\n",
    "    episodes = []\n",
    "    for _ in range(num_episodes):\n",
    "        episode = generate_episode(env, policy, num_days)\n",
    "        episodes.append(episode)\n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b2fe8237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Episodio 1 ---\n",
      "Día 1: Estado={'product_A': 4, 'product_B': 9}, Acción={'product_A': 0, 'product_B': 1}, Recompensa=28\n",
      "Día 2: Estado={'product_A': 4, 'product_B': 9}, Acción={'product_A': 2, 'product_B': 3}, Recompensa=-21\n",
      "Día 3: Estado={'product_A': 5, 'product_B': 10}, Acción={'product_A': 0, 'product_B': 2}, Recompensa=-14\n",
      "Día 4: Estado={'product_A': 5, 'product_B': 10}, Acción={'product_A': 0, 'product_B': 1}, Recompensa=-7\n",
      "Día 5: Estado={'product_A': 5, 'product_B': 10}, Acción={'product_A': 1, 'product_B': 2}, Recompensa=6\n",
      "\n",
      "--- Episodio 2 ---\n",
      "Día 1: Estado={'product_A': 4, 'product_B': 6}, Acción={'product_A': 0, 'product_B': 3}, Recompensa=-6\n",
      "Día 2: Estado={'product_A': 4, 'product_B': 6}, Acción={'product_A': 3, 'product_B': 2}, Recompensa=6\n",
      "Día 3: Estado={'product_A': 5, 'product_B': 7}, Acción={'product_A': 3, 'product_B': 0}, Recompensa=20\n",
      "Día 4: Estado={'product_A': 6, 'product_B': 6}, Acción={'product_A': 3, 'product_B': 3}, Recompensa=-1\n",
      "Día 5: Estado={'product_A': 7, 'product_B': 8}, Acción={'product_A': 0, 'product_B': 1}, Recompensa=3\n",
      "\n",
      "--- Episodio 3 ---\n",
      "Día 1: Estado={'product_A': 7, 'product_B': 4}, Acción={'product_A': 3, 'product_B': 2}, Recompensa=-4\n",
      "Día 2: Estado={'product_A': 7, 'product_B': 4}, Acción={'product_A': 3, 'product_B': 2}, Recompensa=1\n",
      "Día 3: Estado={'product_A': 10, 'product_B': 4}, Acción={'product_A': 0, 'product_B': 1}, Recompensa=-7\n",
      "Día 4: Estado={'product_A': 10, 'product_B': 5}, Acción={'product_A': 2, 'product_B': 0}, Recompensa=0\n",
      "Día 5: Estado={'product_A': 9, 'product_B': 5}, Acción={'product_A': 2, 'product_B': 1}, Recompensa=-7\n"
     ]
    }
   ],
   "source": [
    "env = InventoryEnvironment()\n",
    "episodes = generate_episodes(env, random_policy, num_episodes=3, num_days=5)\n",
    "\n",
    "for i, ep in enumerate(episodes):\n",
    "    print(f\"\\n--- Episodio {i+1} ---\")\n",
    "    for day, (state, action, reward) in enumerate(ep):\n",
    "        print(f\"Día {day+1}: Estado={state}, Acción={action}, Recompensa={reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be23d661",
   "metadata": {},
   "source": [
    "**Exploring Starts**\n",
    "- Implemente explorar inicios para garantizar un conjunto diverso de estados y acciones iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7dd9d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_exploring_starts(env, policy, num_days=10):\n",
    "    # Estado inicial aleatorio\n",
    "    state = {\n",
    "        product: random.randint(0, env.max_stock)\n",
    "        for product in env.products\n",
    "    }\n",
    "    env.state = state.copy()\n",
    "\n",
    "    episode = []\n",
    "\n",
    "    for day in range(num_days):\n",
    "        # Acción inicial aleatoria solo en el primer paso\n",
    "        if day == 0:\n",
    "            action = {\n",
    "                product: random.randint(0, 3)\n",
    "                for product in env.products\n",
    "            }\n",
    "        else:\n",
    "            action = policy(state)\n",
    "\n",
    "        next_state, reward = env.step(action)\n",
    "        episode.append((state.copy(), action.copy(), reward))\n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5f1c72a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episodes_exploring_starts(env, policy, num_episodes=5, num_days=7):\n",
    "    episodes = []\n",
    "    for _ in range(num_episodes):\n",
    "        episode = generate_episode_exploring_starts(env, policy, num_days)\n",
    "        episodes.append(episode)\n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55d95aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Episodio (Exploring Starts) 1 ---\n",
      "Día 1: Estado={'product_A': 2, 'product_B': 4}, Acción={'product_A': 3, 'product_B': 1}, Recompensa=-22\n",
      "Día 2: Estado={'product_A': 5, 'product_B': 5}, Acción={'product_A': 0, 'product_B': 2}, Recompensa=6\n",
      "Día 3: Estado={'product_A': 3, 'product_B': 7}, Acción={'product_A': 1, 'product_B': 2}, Recompensa=11\n",
      "Día 4: Estado={'product_A': 4, 'product_B': 7}, Acción={'product_A': 3, 'product_B': 3}, Recompensa=-1\n",
      "Día 5: Estado={'product_A': 5, 'product_B': 9}, Acción={'product_A': 3, 'product_B': 0}, Recompensa=25\n",
      "\n",
      "--- Episodio (Exploring Starts) 2 ---\n",
      "Día 1: Estado={'product_A': 6, 'product_B': 8}, Acción={'product_A': 0, 'product_B': 1}, Recompensa=8\n",
      "Día 2: Estado={'product_A': 6, 'product_B': 8}, Acción={'product_A': 0, 'product_B': 0}, Recompensa=20\n",
      "Día 3: Estado={'product_A': 4, 'product_B': 8}, Acción={'product_A': 3, 'product_B': 0}, Recompensa=5\n",
      "Día 4: Estado={'product_A': 5, 'product_B': 8}, Acción={'product_A': 1, 'product_B': 2}, Recompensa=6\n",
      "Día 5: Estado={'product_A': 5, 'product_B': 9}, Acción={'product_A': 2, 'product_B': 1}, Recompensa=-2\n",
      "\n",
      "--- Episodio (Exploring Starts) 3 ---\n",
      "Día 1: Estado={'product_A': 10, 'product_B': 0}, Acción={'product_A': 0, 'product_B': 3}, Recompensa=-6\n",
      "Día 2: Estado={'product_A': 10, 'product_B': 2}, Acción={'product_A': 1, 'product_B': 0}, Recompensa=10\n",
      "Día 3: Estado={'product_A': 10, 'product_B': 1}, Acción={'product_A': 3, 'product_B': 0}, Recompensa=-15\n",
      "Día 4: Estado={'product_A': 10, 'product_B': 1}, Acción={'product_A': 3, 'product_B': 0}, Recompensa=10\n",
      "Día 5: Estado={'product_A': 9, 'product_B': 0}, Acción={'product_A': 3, 'product_B': 1}, Recompensa=13\n"
     ]
    }
   ],
   "source": [
    "episodes_es = generate_episodes_exploring_starts(env, random_policy, num_episodes=3, num_days=5)\n",
    "\n",
    "for i, ep in enumerate(episodes_es):\n",
    "    print(f\"\\n--- Episodio (Exploring Starts) {i+1} ---\")\n",
    "    for day, (state, action, reward) in enumerate(ep):\n",
    "        print(f\"Día {day+1}: Estado={state}, Acción={action}, Recompensa={reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d3fed",
   "metadata": {},
   "source": [
    "**Soft Policies**\n",
    "- Utilice una soft policy (como epsilon-greedy) para garantizar un equilibrio entre la exploración y la\n",
    "explotación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e5868024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Episodio (Soft Policy) 1 ---\n",
      "Día 1: Estado={'product_A': 4, 'product_B': 9}, Acción={'product_A': 3, 'product_B': 1}, Recompensa=3\n",
      "Día 2: Estado={'product_A': 4, 'product_B': 9}, Acción={'product_A': 1, 'product_B': 1}, Recompensa=-2\n",
      "Día 3: Estado={'product_A': 4, 'product_B': 10}, Acción={'product_A': 0, 'product_B': 0}, Recompensa=30\n",
      "Día 4: Estado={'product_A': 4, 'product_B': 8}, Acción={'product_A': 3, 'product_B': 0}, Recompensa=5\n",
      "Día 5: Estado={'product_A': 5, 'product_B': 8}, Acción={'product_A': 3, 'product_B': 2}, Recompensa=-9\n",
      "\n",
      "--- Episodio (Soft Policy) 2 ---\n",
      "Día 1: Estado={'product_A': 4, 'product_B': 0}, Acción={'product_A': 0, 'product_B': 2}, Recompensa=26\n",
      "Día 2: Estado={'product_A': 4, 'product_B': 0}, Acción={'product_A': 0, 'product_B': 0}, Recompensa=20\n",
      "Día 3: Estado={'product_A': 2, 'product_B': 0}, Acción={'product_A': 3, 'product_B': 0}, Recompensa=-15\n",
      "Día 4: Estado={'product_A': 5, 'product_B': 0}, Acción={'product_A': 0, 'product_B': 3}, Recompensa=-1\n",
      "Día 5: Estado={'product_A': 3, 'product_B': 3}, Acción={'product_A': 1, 'product_B': 2}, Recompensa=11\n",
      "\n",
      "--- Episodio (Soft Policy) 3 ---\n",
      "Día 1: Estado={'product_A': 4, 'product_B': 10}, Acción={'product_A': 3, 'product_B': 3}, Recompensa=-16\n",
      "Día 2: Estado={'product_A': 4, 'product_B': 10}, Acción={'product_A': 0, 'product_B': 0}, Recompensa=20\n",
      "Día 3: Estado={'product_A': 2, 'product_B': 10}, Acción={'product_A': 0, 'product_B': 1}, Recompensa=-7\n",
      "Día 4: Estado={'product_A': 2, 'product_B': 10}, Acción={'product_A': 3, 'product_B': 1}, Recompensa=-22\n",
      "Día 5: Estado={'product_A': 5, 'product_B': 10}, Acción={'product_A': 3, 'product_B': 0}, Recompensa=10\n",
      "\n",
      "--- Episodio (Soft Policy) 4 ---\n",
      "Día 1: Estado={'product_A': 4, 'product_B': 3}, Acción={'product_A': 0, 'product_B': 2}, Recompensa=36\n",
      "Día 2: Estado={'product_A': 4, 'product_B': 3}, Acción={'product_A': 3, 'product_B': 0}, Recompensa=20\n",
      "Día 3: Estado={'product_A': 5, 'product_B': 2}, Acción={'product_A': 3, 'product_B': 1}, Recompensa=18\n",
      "Día 4: Estado={'product_A': 7, 'product_B': 1}, Acción={'product_A': 3, 'product_B': 1}, Recompensa=13\n",
      "Día 5: Estado={'product_A': 8, 'product_B': 1}, Acción={'product_A': 3, 'product_B': 0}, Recompensa=5\n",
      "\n",
      "--- Episodio (Soft Policy) 5 ---\n",
      "Día 1: Estado={'product_A': 8, 'product_B': 7}, Acción={'product_A': 0, 'product_B': 1}, Recompensa=28\n",
      "Día 2: Estado={'product_A': 8, 'product_B': 7}, Acción={'product_A': 1, 'product_B': 1}, Recompensa=-12\n",
      "Día 3: Estado={'product_A': 9, 'product_B': 8}, Acción={'product_A': 2, 'product_B': 3}, Recompensa=-11\n",
      "Día 4: Estado={'product_A': 8, 'product_B': 10}, Acción={'product_A': 3, 'product_B': 1}, Recompensa=18\n",
      "Día 5: Estado={'product_A': 9, 'product_B': 8}, Acción={'product_A': 1, 'product_B': 3}, Recompensa=-16\n"
     ]
    }
   ],
   "source": [
    "Q = {} # Q-table para almacenar valores de acción-estado = [list of returns]\n",
    "\n",
    "def epsilon_greedy_policy(Q, state, epsilon=0.1):\n",
    "    state_key = str(state)\n",
    "    if state_key not in Q or random.random() < epsilon:\n",
    "        # Seleccionar acción aleatoria\n",
    "        return {\n",
    "            'product_A': random.randint(0, 3),\n",
    "            'product_B': random.randint(0, 3)\n",
    "        }\n",
    "    else:\n",
    "        # Seleccionar la mejor acción basada en Q\n",
    "        best_action = max(Q[state_key], key=lambda a: np.mean(Q[state_key][a]))\n",
    "        return eval(best_action)\n",
    "\n",
    "def generate_episodes_soft_policy(env, Q, num_episodes=5, num_days=7, epsilon=0.1):\n",
    "    episodes = []\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode = []\n",
    "        for _ in range(num_days):\n",
    "            action = epsilon_greedy_policy(Q, state, epsilon)\n",
    "            next_state, reward = env.step(action)\n",
    "            episode.append((state.copy(), action.copy(), reward))\n",
    "            state = next_state\n",
    "        episodes.append(episode)\n",
    "    return episodes\n",
    "\n",
    "def update_Q(Q, episodes):\n",
    "    for episode in episodes:\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for state, action, reward in reversed(episode):\n",
    "            G += reward\n",
    "            state_key = str(state)\n",
    "            action_key = str(action)\n",
    "\n",
    "            if (state_key, action_key) not in visited:\n",
    "                visited.add((state_key, action_key))\n",
    "                if state_key not in Q:\n",
    "                    Q[state_key] = {}\n",
    "                if action_key not in Q[state_key]:\n",
    "                    Q[state_key][action_key] = []\n",
    "                Q[state_key][action_key].append(G)\n",
    "\n",
    "\n",
    "Q = {}\n",
    "episodes_soft = generate_episodes_soft_policy(env, Q, epsilon=0.1, num_episodes=5, num_days=5)\n",
    "update_Q(Q, episodes_soft)\n",
    "\n",
    "# Ver resultados\n",
    "for i, ep in enumerate(episodes_soft):\n",
    "    print(f\"\\n--- Episodio (Soft Policy) {i+1} ---\")\n",
    "    for day, (state, action, reward) in enumerate(ep):\n",
    "        print(f\"Día {day+1}: Estado={state}, Acción={action}, Recompensa={reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e209b7",
   "metadata": {},
   "source": [
    "**Aprendizaje off-policy**\n",
    "- Implemente el aprendizaje off-policy para evaluar una política objetivo utilizando datos generados\n",
    "por una política de comportamiento diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c32838af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores estimados V^pi para algunos estados:\n",
      "{'product_A': 10, 'product_B': 10} -> -12.00\n"
     ]
    }
   ],
   "source": [
    "def target_policy(state):\n",
    "    return {'product_A': 1, 'product_B': 1}\n",
    "\n",
    "action_space = [{'product_A': a, 'product_B': b} for a in range(0, 4) for b in range(0, 4)]\n",
    "n_actions = len(action_space)\n",
    "\n",
    "def behavior_policy(state):\n",
    "    action = random.choice(action_space)\n",
    "    prob_b = 1.0 / n_actions\n",
    "    return action, prob_b\n",
    "\n",
    "def generate_episodes_off_policy(env, num_episodes=10, num_days=10):\n",
    "    episodes = []\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode = []\n",
    "        for _ in range(num_days):\n",
    "            action, prob_b = behavior_policy(state)\n",
    "            next_state, reward = env.step(action)\n",
    "            episode.append((state.copy(), action.copy(), reward, prob_b))\n",
    "            state = next_state\n",
    "        episodes.append(episode)\n",
    "    return episodes\n",
    "\n",
    "def off_policy_evaluation(env, episodes, gamma=1.0):\n",
    "    valores = {}\n",
    "    pesos = {}\n",
    "\n",
    "    for episodio in episodes:\n",
    "        G = 0\n",
    "        W = 1\n",
    "        for state, action, reward, prob_b in reversed(episodio):\n",
    "            G = gamma * G + reward\n",
    "\n",
    "            # Probabilidad según la política objetivo\n",
    "            if action == target_policy(state):\n",
    "                prob_pi = 1.0\n",
    "            else:\n",
    "                prob_pi = 0.0\n",
    "\n",
    "            if prob_b > 0:\n",
    "                W = W * (prob_pi / prob_b)\n",
    "            else:\n",
    "                W = 0\n",
    "\n",
    "            if W == 0:\n",
    "                break\n",
    "\n",
    "            clave_estado = str(state)\n",
    "\n",
    "            # Acumular sumas\n",
    "            if clave_estado not in valores:\n",
    "                valores[clave_estado] = 0\n",
    "                pesos[clave_estado] = 0\n",
    "\n",
    "            valores[clave_estado] = valores[clave_estado] + (W * G)\n",
    "            pesos[clave_estado] = pesos[clave_estado] + W\n",
    "\n",
    "    # Calcular promedio manualmente\n",
    "    V_final = {}\n",
    "    for estado in valores:\n",
    "        if pesos[estado] > 0:\n",
    "            promedio = valores[estado] / pesos[estado]\n",
    "            V_final[estado] = promedio\n",
    "\n",
    "    return V_final\n",
    "\n",
    "episodios_off = generate_episodes_off_policy(env, num_episodes=20, num_days=10)\n",
    "V_pi = off_policy_evaluation(env, episodios_off)\n",
    "\n",
    "print(\"Valores estimados V^pi para algunos estados:\")\n",
    "contador = 0\n",
    "for estado, valor in V_pi.items():\n",
    "    print(f\"{estado} -> {valor:.2f}\")\n",
    "    contador += 1\n",
    "    if contador >= 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5044bc9",
   "metadata": {},
   "source": [
    "1. ¿Cuál es el valor estimado de mantener diferentes niveles de existencias para cada producto?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "09b528fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor estimado por nivel (product_A): {0: 70.0, 1: 51.666666666666664, 2: 58.925925925925924, 3: 79.08, 4: 56.035714285714285, 5: 65.83333333333333, 6: 100.53571428571429, 7: 70.48148148148148, 8: 68.1063829787234, 9: 47.83783783783784, 10: 43.142857142857146}\n",
      "Valor estimado por nivel (product_B): {0: 36.6875, 1: 57.2, 2: 47.89473684210526, 3: 80.76666666666667, 4: 67.22727272727273, 5: 58.851851851851855, 6: 72.91304347826087, 7: 66.33333333333333, 8: 72.05555555555556, 9: 68.41666666666667, 10: 58.65217391304348}\n"
     ]
    }
   ],
   "source": [
    "def valorPorNivelInicial(episodes):\n",
    "    bucketsA, bucketsB = {}, {}\n",
    "    for ep in episodes:\n",
    "        G = sum(r for _,_,r in ep)\n",
    "        s0 = ep[0][0] \n",
    "        lvlA = s0['product_A']\n",
    "        lvlB = s0['product_B']\n",
    "        bucketsA.setdefault(lvlA, []).append(G)\n",
    "        bucketsB.setdefault(lvlB, []).append(G)\n",
    "    promA = {k: sum(v)/len(v) for k,v in bucketsA.items()}\n",
    "    promB = {k: sum(v)/len(v) for k,v in bucketsB.items()}\n",
    "    return dict(sorted(promA.items())), dict(sorted(promB.items()))\n",
    "\n",
    "Q = {}\n",
    "episodes_soft = generate_episodes_soft_policy(env, Q, epsilon=0.2, num_episodes=300, num_days=10)\n",
    "valorA, valorB = valorPorNivelInicial(episodes_soft)\n",
    "print(\"Valor estimado por nivel (product_A):\", valorA)\n",
    "print(\"Valor estimado por nivel (product_B):\", valorB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd7fb73",
   "metadata": {},
   "source": [
    "Con una política blanda de 𝜖=0.2 se calculó el promedio de retorno según el stock inicial de cada producto. Para product_A, el nivel que dio el valor más alto fue el 6 de aproximadamente 100.53, seguido por el 3 de 79.08. Niveles muy bajos como 1 o muy altos como 10 dieron resultados más bajos, por lo que evidencia que no es lo más recomendable iniciar con esos niveles de stock. Para product_B, el mejor fue el nivel 3 de aproximadamente 80.77 y también funcionaron bien los niveles 6 y 8 ambos de aproximadamente 72. En este caso, también iniciar en 0 o 10 no es tan viable. De esta forma se determina contar con un stock inicial “medio” da mejores ganancias, porque hay suficiente producto para vender sin que se dispare el costo de reposición."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86bd643",
   "metadata": {},
   "source": [
    "2. ¿Cómo afecta el valor epsilon en la política blanda al rendimiento?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "13934b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retorno promedio por ε: {0.0: 66.27, 0.1: 62.07, 0.2: 64.06, 0.3: 60.78, 0.5: 65.815}\n"
     ]
    }
   ],
   "source": [
    "def retornoPromedioSoft(env, epsilon, num_episodes=200, num_days=10):\n",
    "    Qtmp = {}\n",
    "    eps = generate_episodes_soft_policy(env, Qtmp, epsilon=epsilon,\n",
    "                                        num_episodes=num_episodes, num_days=num_days)\n",
    "    return sum(sum(r for _,_,r in ep) for ep in eps) / len(eps)\n",
    "\n",
    "epsilons = [0.0, 0.1, 0.2, 0.3, 0.5]\n",
    "resultados = {e: retornoPromedioSoft(env, e) for e in epsilons}\n",
    "print(\"Retorno promedio por ε:\", resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50be27fe",
   "metadata": {},
   "source": [
    "Al variar ε, los resultados fueron bastante parecidos, pero el mejor promedio se obtuvo con ε = 0.0 de 66.27. Con valores entre 0.1 y 0.3 el retorno bajó un poco, y con 0.5 volvió a subir pero sin superar el mejor resultado. Esto indica que, explorar más no ayudó mucho porque la política ya funcionaba bien sin exploración."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f86034b",
   "metadata": {},
   "source": [
    "3. ¿Cuál es el impacto de utilizar el aprendizaje fuera de la política en comparación con el aprendizaje dentro de la política?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9996caa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ON-policy - promedio de V(s): 75.85\n",
      "OFF-policy - promedio de V^π(s): 14.06 -> estados: 16\n"
     ]
    }
   ],
   "source": [
    "Q_tmp = {}\n",
    "eps_soft_local = generate_episodes_soft_policy(env, Q_tmp, epsilon=0.2,\n",
    "                                               num_episodes=200, num_days=10)\n",
    "\n",
    "valores = {}\n",
    "conteos = {}\n",
    "\n",
    "for ep in eps_soft_local:\n",
    "    G = 0.0\n",
    "    for _, _, r in ep:\n",
    "        G += r\n",
    "    s0 = str(ep[0][0]) \n",
    "    if s0 not in valores:\n",
    "        valores[s0] = 0.0\n",
    "        conteos[s0] = 0\n",
    "    valores[s0] += G\n",
    "    conteos[s0] += 1\n",
    "\n",
    "V_on = {}\n",
    "for s in valores:\n",
    "    V_on[s] = valores[s] / conteos[s]\n",
    "\n",
    "on_state_avg = sum(V_on.values()) / len(V_on) if V_on else 0.0\n",
    "off_state_avg = (sum(V_pi.values()) / len(V_pi)) if V_pi else 0.0\n",
    "\n",
    "print(f\"ON-policy - promedio de V(s): {on_state_avg:.2f}\")\n",
    "print(f\"OFF-policy - promedio de V^π(s): {off_state_avg:.2f} -> estados: {len(V_pi)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8284dc24",
   "metadata": {},
   "source": [
    "En on-policy (ε = 0.2) obtuve un retorno promedio de 75.85. En off-policy, el promedio estimado de 𝑉^𝜋(𝑠) fue de 14.06, calculado sobre 16 estados. Esta gran diferencia se debe a que en off-policy los episodios se generan con una política de comportamiento distinta a la política objetivo. Como la política objetivo en este caso es determinista, solo los episodios que coinciden exactamente con sus acciones aportan valor al cálculo, y de estos, fueron pocos los que coincidieron. Esto provoca que el valor promedio calculado sea muy bajo y con más variación. En cambio, on-policy siempre evalúa la misma política que está usando para actuar, por lo que aprovecha todos los episodios y el valor promedio es más alto y estable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
