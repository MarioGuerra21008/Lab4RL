{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c5ba4e",
   "metadata": {},
   "source": [
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <td style=\"width:20%; vertical-align:middle;\">\n",
    "      <img src=\"LogoUVG.png\" width=\"400\"/>\n",
    "    </td>\n",
    "    <td style=\"text-align:left; vertical-align:middle;\">\n",
    "      <h2 style=\"margin-bottom: 0;\">Universidad del Valle de Guatemala - UVG</h2>\n",
    "      <h3 style=\"margin-top: 0;\">Facultad de Ingenier√≠a - Computaci√≥n</h3>\n",
    "      <p style=\"font-size: 16px; margin-bottom: 0; margin-top: -20px\">\n",
    "        <strong>Curso:</strong> CC3104 - Aprendizaje por Refuerzo \n",
    "        <strong>Secci√≥n:</strong> 10\n",
    "      </p>\n",
    "      <p style=\"font-size: 16px; margin: 0;\"><strong>Laboratorio 4:</strong> M√©todos de Monte Carlo</p>\n",
    "      <br>\n",
    "      <p style=\"font-size: 15px; margin: 0;\"><strong>Autores:</strong></p>\n",
    "      <ul style=\"margin-top: 5px; padding-left: 20px; font-size: 15px;\">\n",
    "        <li>Diego Alexander Hern√°ndez Silvestre - <strong>21270</strong></li>\n",
    "        <li>Linda In√©s Jim√©nez Vides - <strong>21169</strong></li>\n",
    "        <li>Mario Antonio Guerra Morales - <strong>21008</strong></li>\n",
    "      </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e0934d",
   "metadata": {},
   "source": [
    "## üìù Task 1\n",
    "\n",
    "**1. ¬øC√≥mo afecta la elecci√≥n de la estrategia de exploraci√≥n (exploring starts vs soft policy) a la precisi√≥n de la evaluaci√≥n de pol√≠ticas en los m√©todos de Monte Carlo? Considere la posibilidad de comparar el desempe√±o de las pol√≠ticas evaluadas con y sin explorar los inicios o con diferentes niveles de exploraci√≥n en pol√≠ticas blandas.**\n",
    "\n",
    "- La elecci√≥n de estrategia de exploraci√≥n s√≠ tiene un impacto directo en la evaluaci√≥n de pol√≠ticas al usar los m√©todos de Monte Carlo. Si se utiliza Exploring Starts, se garantiza una mayor diversidad en las trayectorias debido a que se puede iniciar desde distintas combinaciones de estados y de acciones. Mientras que, con Soft Policies se a√±ade un equilibrio entre la exploraci√≥n y la explotaci√≥n. Cuando se mantiene una probabilidad de selecci√≥n de acciones aleatorias, se a√±ade la posibilidad de seguir explorando nuevas y aleatorias trayectorias, a√∫n priorizando acciones de mayor valor.\n",
    "\n",
    "**2. En el contexto del aprendizaje de Monte Carlo fuera de la p√≥liza, ¬øc√≥mo afecta la raz√≥n de muestreo de importancia a la convergencia de la evaluaci√≥n de pol√≠ticas? Explore c√≥mo la raz√≥n de muestreo de importancia afecta la estabilidad y la convergencia.**\n",
    "\n",
    "- La raz√≥n de muestreo de importancia ajusta las estimaciones de retorno para que la pol√≠tica objetivo se refleje utilizando datos de manera diferente. Esta afecta al permitirse aprender sobre una pol√≠tica objetivo sin hacer un seguimiento directo, pero tambi√©n es capaz de hacer inestables las razones si tanto el comportamiento como el objetivo difieren demasiado.\n",
    "\n",
    "**3. ¬øC√≥mo puede el uso de una soft policy influir en la eficacia del aprendizaje de pol√≠ticas √≥ptimas en comparaci√≥n con las pol√≠ticas deterministas en los m√©todos de Monte Carlo? Compare el desempe√±o y los resultados de aprendizaje de las pol√≠ticas derivadas de estrategias √©psilon-greedy con las derivadas de pol√≠ticas deterministas.**\n",
    "\n",
    "- Una soft policy puede influir con una mayor exploraci√≥n, esto para ayudar al descubrimiento de acciones que no son efectivas a corto plazo pero s√≠ a un largo plazo. Mientras que, una pol√≠tica determinista elegir√° la misma acci√≥n siempre, lo que puede llegar a darse el riesgo de no hacer una correcta exploraci√≥n.\n",
    "\n",
    "**4. ¬øCu√°les son los posibles beneficios y desventajas de utilizar m√©todos de Monte Carlo off-policy en comparaci√≥n con los on-policy en t√©rminos de eficiencia de la muestra, costo computacional y velocidad de aprendizaje?**\n",
    "\n",
    "- Los m√©todos de Monte Carlo off-policy ofrecen una mejor eficiencia al existir la posibilidad de usar datos previos o simulados, pero ello contempla un mayor costo computacional. Sin embargo, esto a su vez permite una gran flexibilidad y con ello una velocidad de aprendizaje considerable, si es que las pol√≠ticas no difieren mucho con el objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1376915",
   "metadata": {},
   "source": [
    "## üìù Task 2\n",
    "\n",
    "En este ejercicio, simular√° un sistema de gesti√≥n de inventarios para una peque√±a tienda minorista. La tienda tiene como objetivo maximizar las ganancias manteniendo niveles √≥ptimos de existencias de diferentes productos. Utilizar√° m√©todos de Monte Carlo para la evaluaci√≥n de p√≥lizas, exploring starts, soft policies y aprendizaje off-policy para estimar el valor de diferentes estrategias de gesti√≥n de inventarios. Su objetivo es implementar una soluci√≥n en Python y responder preguntas espec√≠ficas en funci√≥n de los resultados.\n",
    "\n",
    "**Definici√≥n del entorno**\n",
    "- Utilice el ambiente dado m√°s adelante para simular el entorno de la tienda. Considere que:\n",
    "    - El estado representa los niveles de existencias actuales de los productos.\n",
    "    - Las acciones representan decisiones sobre cu√°nto reponer de cada producto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "396c0503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class InventoryEnvironment:\n",
    "    def __init__(self):\n",
    "        self.products = ['product_A', 'product_B']\n",
    "        self.max_stock = 10\n",
    "        self.demand = {'product_A': [0, 1, 2], 'product_B': [0, 1, 2]}\n",
    "        self.restock_cost = {'product_A': 5, 'product_B': 7}\n",
    "        self.sell_price = {'product_A': 10, 'product_B': 15}\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = {product: random.randint(0, self.max_stock) for product in self.products}\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        for product in self.products:\n",
    "            stock = self.state[product]\n",
    "            restock = action[product]\n",
    "            self.state[product] = min(self.max_stock, stock + restock)\n",
    "            demand = random.choice(self.demand[product])\n",
    "            sales = min(demand, self.state[product])\n",
    "            self.state[product] -= sales\n",
    "            reward += sales * self.sell_price[product] - restock * self.restock_cost[product]\n",
    "        return self.state.copy(), reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034ae583",
   "metadata": {},
   "source": [
    "**Generaci√≥n de episodios**\n",
    "- Cada episodio representa una serie de d√≠as en los que la tienda sigue una pol√≠tica de inventario espec√≠fica.\n",
    "- Debe recopilar datos para varios episodios y registrar las recompensas (ganancias) de cada d√≠a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "99c17973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, policy, num_days=10):\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "\n",
    "    for _ in range(num_days):\n",
    "        # Elegir acci√≥n seg√∫n la pol√≠tica\n",
    "        action = policy(state)\n",
    "        # Ejecutar acci√≥n en el entorno\n",
    "        next_state, reward = env.step(action)\n",
    "        # Guardar transici√≥n: estado, acci√≥n, recompensa\n",
    "        episode.append((state.copy(), action.copy(), reward))\n",
    "        # Actualizar estado actual\n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9281d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    return {\n",
    "        'product_A': random.randint(0, 3),\n",
    "        'product_B': random.randint(0, 3)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4b45f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episodes(env, policy, num_episodes=100, num_days=10):\n",
    "    episodes = []\n",
    "    for _ in range(num_episodes):\n",
    "        episode = generate_episode(env, policy, num_days)\n",
    "        episodes.append(episode)\n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b2fe8237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Episodio 1 ---\n",
      "D√≠a 1: Estado={'product_A': 4, 'product_B': 9}, Acci√≥n={'product_A': 0, 'product_B': 1}, Recompensa=28\n",
      "D√≠a 2: Estado={'product_A': 4, 'product_B': 9}, Acci√≥n={'product_A': 2, 'product_B': 3}, Recompensa=-21\n",
      "D√≠a 3: Estado={'product_A': 5, 'product_B': 10}, Acci√≥n={'product_A': 0, 'product_B': 2}, Recompensa=-14\n",
      "D√≠a 4: Estado={'product_A': 5, 'product_B': 10}, Acci√≥n={'product_A': 0, 'product_B': 1}, Recompensa=-7\n",
      "D√≠a 5: Estado={'product_A': 5, 'product_B': 10}, Acci√≥n={'product_A': 1, 'product_B': 2}, Recompensa=6\n",
      "\n",
      "--- Episodio 2 ---\n",
      "D√≠a 1: Estado={'product_A': 4, 'product_B': 6}, Acci√≥n={'product_A': 0, 'product_B': 3}, Recompensa=-6\n",
      "D√≠a 2: Estado={'product_A': 4, 'product_B': 6}, Acci√≥n={'product_A': 3, 'product_B': 2}, Recompensa=6\n",
      "D√≠a 3: Estado={'product_A': 5, 'product_B': 7}, Acci√≥n={'product_A': 3, 'product_B': 0}, Recompensa=20\n",
      "D√≠a 4: Estado={'product_A': 6, 'product_B': 6}, Acci√≥n={'product_A': 3, 'product_B': 3}, Recompensa=-1\n",
      "D√≠a 5: Estado={'product_A': 7, 'product_B': 8}, Acci√≥n={'product_A': 0, 'product_B': 1}, Recompensa=3\n",
      "\n",
      "--- Episodio 3 ---\n",
      "D√≠a 1: Estado={'product_A': 7, 'product_B': 4}, Acci√≥n={'product_A': 3, 'product_B': 2}, Recompensa=-4\n",
      "D√≠a 2: Estado={'product_A': 7, 'product_B': 4}, Acci√≥n={'product_A': 3, 'product_B': 2}, Recompensa=1\n",
      "D√≠a 3: Estado={'product_A': 10, 'product_B': 4}, Acci√≥n={'product_A': 0, 'product_B': 1}, Recompensa=-7\n",
      "D√≠a 4: Estado={'product_A': 10, 'product_B': 5}, Acci√≥n={'product_A': 2, 'product_B': 0}, Recompensa=0\n",
      "D√≠a 5: Estado={'product_A': 9, 'product_B': 5}, Acci√≥n={'product_A': 2, 'product_B': 1}, Recompensa=-7\n"
     ]
    }
   ],
   "source": [
    "env = InventoryEnvironment()\n",
    "episodes = generate_episodes(env, random_policy, num_episodes=3, num_days=5)\n",
    "\n",
    "for i, ep in enumerate(episodes):\n",
    "    print(f\"\\n--- Episodio {i+1} ---\")\n",
    "    for day, (state, action, reward) in enumerate(ep):\n",
    "        print(f\"D√≠a {day+1}: Estado={state}, Acci√≥n={action}, Recompensa={reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be23d661",
   "metadata": {},
   "source": [
    "**Exploring Starts**\n",
    "- Implemente explorar inicios para garantizar un conjunto diverso de estados y acciones iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7dd9d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_exploring_starts(env, policy, num_days=10):\n",
    "    # Estado inicial aleatorio\n",
    "    state = {\n",
    "        product: random.randint(0, env.max_stock)\n",
    "        for product in env.products\n",
    "    }\n",
    "    env.state = state.copy()\n",
    "\n",
    "    episode = []\n",
    "\n",
    "    for day in range(num_days):\n",
    "        # Acci√≥n inicial aleatoria solo en el primer paso\n",
    "        if day == 0:\n",
    "            action = {\n",
    "                product: random.randint(0, 3)\n",
    "                for product in env.products\n",
    "            }\n",
    "        else:\n",
    "            action = policy(state)\n",
    "\n",
    "        next_state, reward = env.step(action)\n",
    "        episode.append((state.copy(), action.copy(), reward))\n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5f1c72a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episodes_exploring_starts(env, policy, num_episodes=5, num_days=7):\n",
    "    episodes = []\n",
    "    for _ in range(num_episodes):\n",
    "        episode = generate_episode_exploring_starts(env, policy, num_days)\n",
    "        episodes.append(episode)\n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55d95aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Episodio (Exploring Starts) 1 ---\n",
      "D√≠a 1: Estado={'product_A': 2, 'product_B': 4}, Acci√≥n={'product_A': 3, 'product_B': 1}, Recompensa=-22\n",
      "D√≠a 2: Estado={'product_A': 5, 'product_B': 5}, Acci√≥n={'product_A': 0, 'product_B': 2}, Recompensa=6\n",
      "D√≠a 3: Estado={'product_A': 3, 'product_B': 7}, Acci√≥n={'product_A': 1, 'product_B': 2}, Recompensa=11\n",
      "D√≠a 4: Estado={'product_A': 4, 'product_B': 7}, Acci√≥n={'product_A': 3, 'product_B': 3}, Recompensa=-1\n",
      "D√≠a 5: Estado={'product_A': 5, 'product_B': 9}, Acci√≥n={'product_A': 3, 'product_B': 0}, Recompensa=25\n",
      "\n",
      "--- Episodio (Exploring Starts) 2 ---\n",
      "D√≠a 1: Estado={'product_A': 6, 'product_B': 8}, Acci√≥n={'product_A': 0, 'product_B': 1}, Recompensa=8\n",
      "D√≠a 2: Estado={'product_A': 6, 'product_B': 8}, Acci√≥n={'product_A': 0, 'product_B': 0}, Recompensa=20\n",
      "D√≠a 3: Estado={'product_A': 4, 'product_B': 8}, Acci√≥n={'product_A': 3, 'product_B': 0}, Recompensa=5\n",
      "D√≠a 4: Estado={'product_A': 5, 'product_B': 8}, Acci√≥n={'product_A': 1, 'product_B': 2}, Recompensa=6\n",
      "D√≠a 5: Estado={'product_A': 5, 'product_B': 9}, Acci√≥n={'product_A': 2, 'product_B': 1}, Recompensa=-2\n",
      "\n",
      "--- Episodio (Exploring Starts) 3 ---\n",
      "D√≠a 1: Estado={'product_A': 10, 'product_B': 0}, Acci√≥n={'product_A': 0, 'product_B': 3}, Recompensa=-6\n",
      "D√≠a 2: Estado={'product_A': 10, 'product_B': 2}, Acci√≥n={'product_A': 1, 'product_B': 0}, Recompensa=10\n",
      "D√≠a 3: Estado={'product_A': 10, 'product_B': 1}, Acci√≥n={'product_A': 3, 'product_B': 0}, Recompensa=-15\n",
      "D√≠a 4: Estado={'product_A': 10, 'product_B': 1}, Acci√≥n={'product_A': 3, 'product_B': 0}, Recompensa=10\n",
      "D√≠a 5: Estado={'product_A': 9, 'product_B': 0}, Acci√≥n={'product_A': 3, 'product_B': 1}, Recompensa=13\n"
     ]
    }
   ],
   "source": [
    "episodes_es = generate_episodes_exploring_starts(env, random_policy, num_episodes=3, num_days=5)\n",
    "\n",
    "for i, ep in enumerate(episodes_es):\n",
    "    print(f\"\\n--- Episodio (Exploring Starts) {i+1} ---\")\n",
    "    for day, (state, action, reward) in enumerate(ep):\n",
    "        print(f\"D√≠a {day+1}: Estado={state}, Acci√≥n={action}, Recompensa={reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d3fed",
   "metadata": {},
   "source": [
    "**Soft Policies**\n",
    "- Utilice una soft policy (como epsilon-greedy) para garantizar un equilibrio entre la exploraci√≥n y la\n",
    "explotaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e5868024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Episodio (Soft Policy) 1 ---\n",
      "D√≠a 1: Estado={'product_A': 4, 'product_B': 9}, Acci√≥n={'product_A': 3, 'product_B': 1}, Recompensa=3\n",
      "D√≠a 2: Estado={'product_A': 4, 'product_B': 9}, Acci√≥n={'product_A': 1, 'product_B': 1}, Recompensa=-2\n",
      "D√≠a 3: Estado={'product_A': 4, 'product_B': 10}, Acci√≥n={'product_A': 0, 'product_B': 0}, Recompensa=30\n",
      "D√≠a 4: Estado={'product_A': 4, 'product_B': 8}, Acci√≥n={'product_A': 3, 'product_B': 0}, Recompensa=5\n",
      "D√≠a 5: Estado={'product_A': 5, 'product_B': 8}, Acci√≥n={'product_A': 3, 'product_B': 2}, Recompensa=-9\n",
      "\n",
      "--- Episodio (Soft Policy) 2 ---\n",
      "D√≠a 1: Estado={'product_A': 4, 'product_B': 0}, Acci√≥n={'product_A': 0, 'product_B': 2}, Recompensa=26\n",
      "D√≠a 2: Estado={'product_A': 4, 'product_B': 0}, Acci√≥n={'product_A': 0, 'product_B': 0}, Recompensa=20\n",
      "D√≠a 3: Estado={'product_A': 2, 'product_B': 0}, Acci√≥n={'product_A': 3, 'product_B': 0}, Recompensa=-15\n",
      "D√≠a 4: Estado={'product_A': 5, 'product_B': 0}, Acci√≥n={'product_A': 0, 'product_B': 3}, Recompensa=-1\n",
      "D√≠a 5: Estado={'product_A': 3, 'product_B': 3}, Acci√≥n={'product_A': 1, 'product_B': 2}, Recompensa=11\n",
      "\n",
      "--- Episodio (Soft Policy) 3 ---\n",
      "D√≠a 1: Estado={'product_A': 4, 'product_B': 10}, Acci√≥n={'product_A': 3, 'product_B': 3}, Recompensa=-16\n",
      "D√≠a 2: Estado={'product_A': 4, 'product_B': 10}, Acci√≥n={'product_A': 0, 'product_B': 0}, Recompensa=20\n",
      "D√≠a 3: Estado={'product_A': 2, 'product_B': 10}, Acci√≥n={'product_A': 0, 'product_B': 1}, Recompensa=-7\n",
      "D√≠a 4: Estado={'product_A': 2, 'product_B': 10}, Acci√≥n={'product_A': 3, 'product_B': 1}, Recompensa=-22\n",
      "D√≠a 5: Estado={'product_A': 5, 'product_B': 10}, Acci√≥n={'product_A': 3, 'product_B': 0}, Recompensa=10\n",
      "\n",
      "--- Episodio (Soft Policy) 4 ---\n",
      "D√≠a 1: Estado={'product_A': 4, 'product_B': 3}, Acci√≥n={'product_A': 0, 'product_B': 2}, Recompensa=36\n",
      "D√≠a 2: Estado={'product_A': 4, 'product_B': 3}, Acci√≥n={'product_A': 3, 'product_B': 0}, Recompensa=20\n",
      "D√≠a 3: Estado={'product_A': 5, 'product_B': 2}, Acci√≥n={'product_A': 3, 'product_B': 1}, Recompensa=18\n",
      "D√≠a 4: Estado={'product_A': 7, 'product_B': 1}, Acci√≥n={'product_A': 3, 'product_B': 1}, Recompensa=13\n",
      "D√≠a 5: Estado={'product_A': 8, 'product_B': 1}, Acci√≥n={'product_A': 3, 'product_B': 0}, Recompensa=5\n",
      "\n",
      "--- Episodio (Soft Policy) 5 ---\n",
      "D√≠a 1: Estado={'product_A': 8, 'product_B': 7}, Acci√≥n={'product_A': 0, 'product_B': 1}, Recompensa=28\n",
      "D√≠a 2: Estado={'product_A': 8, 'product_B': 7}, Acci√≥n={'product_A': 1, 'product_B': 1}, Recompensa=-12\n",
      "D√≠a 3: Estado={'product_A': 9, 'product_B': 8}, Acci√≥n={'product_A': 2, 'product_B': 3}, Recompensa=-11\n",
      "D√≠a 4: Estado={'product_A': 8, 'product_B': 10}, Acci√≥n={'product_A': 3, 'product_B': 1}, Recompensa=18\n",
      "D√≠a 5: Estado={'product_A': 9, 'product_B': 8}, Acci√≥n={'product_A': 1, 'product_B': 3}, Recompensa=-16\n"
     ]
    }
   ],
   "source": [
    "Q = {} # Q-table para almacenar valores de acci√≥n-estado = [list of returns]\n",
    "\n",
    "def epsilon_greedy_policy(Q, state, epsilon=0.1):\n",
    "    state_key = str(state)\n",
    "    if state_key not in Q or random.random() < epsilon:\n",
    "        # Seleccionar acci√≥n aleatoria\n",
    "        return {\n",
    "            'product_A': random.randint(0, 3),\n",
    "            'product_B': random.randint(0, 3)\n",
    "        }\n",
    "    else:\n",
    "        # Seleccionar la mejor acci√≥n basada en Q\n",
    "        best_action = max(Q[state_key], key=lambda a: np.mean(Q[state_key][a]))\n",
    "        return eval(best_action)\n",
    "\n",
    "def generate_episodes_soft_policy(env, Q, num_episodes=5, num_days=7, epsilon=0.1):\n",
    "    episodes = []\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode = []\n",
    "        for _ in range(num_days):\n",
    "            action = epsilon_greedy_policy(Q, state, epsilon)\n",
    "            next_state, reward = env.step(action)\n",
    "            episode.append((state.copy(), action.copy(), reward))\n",
    "            state = next_state\n",
    "        episodes.append(episode)\n",
    "    return episodes\n",
    "\n",
    "def update_Q(Q, episodes):\n",
    "    for episode in episodes:\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for state, action, reward in reversed(episode):\n",
    "            G += reward\n",
    "            state_key = str(state)\n",
    "            action_key = str(action)\n",
    "\n",
    "            if (state_key, action_key) not in visited:\n",
    "                visited.add((state_key, action_key))\n",
    "                if state_key not in Q:\n",
    "                    Q[state_key] = {}\n",
    "                if action_key not in Q[state_key]:\n",
    "                    Q[state_key][action_key] = []\n",
    "                Q[state_key][action_key].append(G)\n",
    "\n",
    "\n",
    "Q = {}\n",
    "episodes_soft = generate_episodes_soft_policy(env, Q, epsilon=0.1, num_episodes=5, num_days=5)\n",
    "update_Q(Q, episodes_soft)\n",
    "\n",
    "# Ver resultados\n",
    "for i, ep in enumerate(episodes_soft):\n",
    "    print(f\"\\n--- Episodio (Soft Policy) {i+1} ---\")\n",
    "    for day, (state, action, reward) in enumerate(ep):\n",
    "        print(f\"D√≠a {day+1}: Estado={state}, Acci√≥n={action}, Recompensa={reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e209b7",
   "metadata": {},
   "source": [
    "**Aprendizaje off-policy**\n",
    "- Implemente el aprendizaje off-policy para evaluar una pol√≠tica objetivo utilizando datos generados\n",
    "por una pol√≠tica de comportamiento diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c32838af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores estimados V^pi para algunos estados:\n",
      "{'product_A': 10, 'product_B': 10} -> -12.00\n"
     ]
    }
   ],
   "source": [
    "def target_policy(state):\n",
    "    return {'product_A': 1, 'product_B': 1}\n",
    "\n",
    "action_space = [{'product_A': a, 'product_B': b} for a in range(0, 4) for b in range(0, 4)]\n",
    "n_actions = len(action_space)\n",
    "\n",
    "def behavior_policy(state):\n",
    "    action = random.choice(action_space)\n",
    "    prob_b = 1.0 / n_actions\n",
    "    return action, prob_b\n",
    "\n",
    "def generate_episodes_off_policy(env, num_episodes=10, num_days=10):\n",
    "    episodes = []\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode = []\n",
    "        for _ in range(num_days):\n",
    "            action, prob_b = behavior_policy(state)\n",
    "            next_state, reward = env.step(action)\n",
    "            episode.append((state.copy(), action.copy(), reward, prob_b))\n",
    "            state = next_state\n",
    "        episodes.append(episode)\n",
    "    return episodes\n",
    "\n",
    "def off_policy_evaluation(env, episodes, gamma=1.0):\n",
    "    valores = {}\n",
    "    pesos = {}\n",
    "\n",
    "    for episodio in episodes:\n",
    "        G = 0\n",
    "        W = 1\n",
    "        for state, action, reward, prob_b in reversed(episodio):\n",
    "            G = gamma * G + reward\n",
    "\n",
    "            # Probabilidad seg√∫n la pol√≠tica objetivo\n",
    "            if action == target_policy(state):\n",
    "                prob_pi = 1.0\n",
    "            else:\n",
    "                prob_pi = 0.0\n",
    "\n",
    "            if prob_b > 0:\n",
    "                W = W * (prob_pi / prob_b)\n",
    "            else:\n",
    "                W = 0\n",
    "\n",
    "            if W == 0:\n",
    "                break\n",
    "\n",
    "            clave_estado = str(state)\n",
    "\n",
    "            # Acumular sumas\n",
    "            if clave_estado not in valores:\n",
    "                valores[clave_estado] = 0\n",
    "                pesos[clave_estado] = 0\n",
    "\n",
    "            valores[clave_estado] = valores[clave_estado] + (W * G)\n",
    "            pesos[clave_estado] = pesos[clave_estado] + W\n",
    "\n",
    "    # Calcular promedio manualmente\n",
    "    V_final = {}\n",
    "    for estado in valores:\n",
    "        if pesos[estado] > 0:\n",
    "            promedio = valores[estado] / pesos[estado]\n",
    "            V_final[estado] = promedio\n",
    "\n",
    "    return V_final\n",
    "\n",
    "episodios_off = generate_episodes_off_policy(env, num_episodes=20, num_days=10)\n",
    "V_pi = off_policy_evaluation(env, episodios_off)\n",
    "\n",
    "print(\"Valores estimados V^pi para algunos estados:\")\n",
    "contador = 0\n",
    "for estado, valor in V_pi.items():\n",
    "    print(f\"{estado} -> {valor:.2f}\")\n",
    "    contador += 1\n",
    "    if contador >= 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5044bc9",
   "metadata": {},
   "source": [
    "1. ¬øCu√°l es el valor estimado de mantener diferentes niveles de existencias para cada producto?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "09b528fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor estimado por nivel (product_A): {0: 70.0, 1: 51.666666666666664, 2: 58.925925925925924, 3: 79.08, 4: 56.035714285714285, 5: 65.83333333333333, 6: 100.53571428571429, 7: 70.48148148148148, 8: 68.1063829787234, 9: 47.83783783783784, 10: 43.142857142857146}\n",
      "Valor estimado por nivel (product_B): {0: 36.6875, 1: 57.2, 2: 47.89473684210526, 3: 80.76666666666667, 4: 67.22727272727273, 5: 58.851851851851855, 6: 72.91304347826087, 7: 66.33333333333333, 8: 72.05555555555556, 9: 68.41666666666667, 10: 58.65217391304348}\n"
     ]
    }
   ],
   "source": [
    "def valorPorNivelInicial(episodes):\n",
    "    bucketsA, bucketsB = {}, {}\n",
    "    for ep in episodes:\n",
    "        G = sum(r for _,_,r in ep)\n",
    "        s0 = ep[0][0] \n",
    "        lvlA = s0['product_A']\n",
    "        lvlB = s0['product_B']\n",
    "        bucketsA.setdefault(lvlA, []).append(G)\n",
    "        bucketsB.setdefault(lvlB, []).append(G)\n",
    "    promA = {k: sum(v)/len(v) for k,v in bucketsA.items()}\n",
    "    promB = {k: sum(v)/len(v) for k,v in bucketsB.items()}\n",
    "    return dict(sorted(promA.items())), dict(sorted(promB.items()))\n",
    "\n",
    "Q = {}\n",
    "episodes_soft = generate_episodes_soft_policy(env, Q, epsilon=0.2, num_episodes=300, num_days=10)\n",
    "valorA, valorB = valorPorNivelInicial(episodes_soft)\n",
    "print(\"Valor estimado por nivel (product_A):\", valorA)\n",
    "print(\"Valor estimado por nivel (product_B):\", valorB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd7fb73",
   "metadata": {},
   "source": [
    "Con una pol√≠tica blanda de ùúñ=0.2 se calcul√≥ el promedio de retorno seg√∫n el stock inicial de cada producto. Para product_A, el nivel que dio el valor m√°s alto fue el 6 de aproximadamente 100.53, seguido por el 3 de 79.08. Niveles muy bajos como 1 o muy altos como 10 dieron resultados m√°s bajos, por lo que evidencia que no es lo m√°s recomendable iniciar con esos niveles de stock. Para product_B, el mejor fue el nivel 3 de aproximadamente 80.77 y tambi√©n funcionaron bien los niveles 6 y 8 ambos de aproximadamente 72. En este caso, tambi√©n iniciar en 0 o 10 no es tan viable. De esta forma se determina contar con un stock inicial ‚Äúmedio‚Äù da mejores ganancias, porque hay suficiente producto para vender sin que se dispare el costo de reposici√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86bd643",
   "metadata": {},
   "source": [
    "2. ¬øC√≥mo afecta el valor epsilon en la pol√≠tica blanda al rendimiento?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "13934b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retorno promedio por Œµ: {0.0: 66.27, 0.1: 62.07, 0.2: 64.06, 0.3: 60.78, 0.5: 65.815}\n"
     ]
    }
   ],
   "source": [
    "def retornoPromedioSoft(env, epsilon, num_episodes=200, num_days=10):\n",
    "    Qtmp = {}\n",
    "    eps = generate_episodes_soft_policy(env, Qtmp, epsilon=epsilon,\n",
    "                                        num_episodes=num_episodes, num_days=num_days)\n",
    "    return sum(sum(r for _,_,r in ep) for ep in eps) / len(eps)\n",
    "\n",
    "epsilons = [0.0, 0.1, 0.2, 0.3, 0.5]\n",
    "resultados = {e: retornoPromedioSoft(env, e) for e in epsilons}\n",
    "print(\"Retorno promedio por Œµ:\", resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50be27fe",
   "metadata": {},
   "source": [
    "Al variar Œµ, los resultados fueron bastante parecidos, pero el mejor promedio se obtuvo con Œµ = 0.0 de 66.27. Con valores entre 0.1 y 0.3 el retorno baj√≥ un poco, y con 0.5 volvi√≥ a subir pero sin superar el mejor resultado. Esto indica que, explorar m√°s no ayud√≥ mucho porque la pol√≠tica ya funcionaba bien sin exploraci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f86034b",
   "metadata": {},
   "source": [
    "3. ¬øCu√°l es el impacto de utilizar el aprendizaje fuera de la pol√≠tica en comparaci√≥n con el aprendizaje dentro de la pol√≠tica?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9996caa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ON-policy - promedio de V(s): 75.85\n",
      "OFF-policy - promedio de V^œÄ(s): 14.06 -> estados: 16\n"
     ]
    }
   ],
   "source": [
    "Q_tmp = {}\n",
    "eps_soft_local = generate_episodes_soft_policy(env, Q_tmp, epsilon=0.2,\n",
    "                                               num_episodes=200, num_days=10)\n",
    "\n",
    "valores = {}\n",
    "conteos = {}\n",
    "\n",
    "for ep in eps_soft_local:\n",
    "    G = 0.0\n",
    "    for _, _, r in ep:\n",
    "        G += r\n",
    "    s0 = str(ep[0][0]) \n",
    "    if s0 not in valores:\n",
    "        valores[s0] = 0.0\n",
    "        conteos[s0] = 0\n",
    "    valores[s0] += G\n",
    "    conteos[s0] += 1\n",
    "\n",
    "V_on = {}\n",
    "for s in valores:\n",
    "    V_on[s] = valores[s] / conteos[s]\n",
    "\n",
    "on_state_avg = sum(V_on.values()) / len(V_on) if V_on else 0.0\n",
    "off_state_avg = (sum(V_pi.values()) / len(V_pi)) if V_pi else 0.0\n",
    "\n",
    "print(f\"ON-policy - promedio de V(s): {on_state_avg:.2f}\")\n",
    "print(f\"OFF-policy - promedio de V^œÄ(s): {off_state_avg:.2f} -> estados: {len(V_pi)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8284dc24",
   "metadata": {},
   "source": [
    "En on-policy (Œµ = 0.2) obtuve un retorno promedio de 75.85. En off-policy, el promedio estimado de ùëâ^ùúã(ùë†) fue de 14.06, calculado sobre 16 estados. Esta gran diferencia se debe a que en off-policy los episodios se generan con una pol√≠tica de comportamiento distinta a la pol√≠tica objetivo. Como la pol√≠tica objetivo en este caso es determinista, solo los episodios que coinciden exactamente con sus acciones aportan valor al c√°lculo, y de estos, fueron pocos los que coincidieron. Esto provoca que el valor promedio calculado sea muy bajo y con m√°s variaci√≥n. En cambio, on-policy siempre eval√∫a la misma pol√≠tica que est√° usando para actuar, por lo que aprovecha todos los episodios y el valor promedio es m√°s alto y estable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
