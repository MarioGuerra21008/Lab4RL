{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c5ba4e",
   "metadata": {},
   "source": [
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <td style=\"width:20%; vertical-align:middle;\">\n",
    "      <img src=\"LogoUVG.png\" width=\"400\"/>\n",
    "    </td>\n",
    "    <td style=\"text-align:left; vertical-align:middle;\">\n",
    "      <h2 style=\"margin-bottom: 0;\">Universidad del Valle de Guatemala - UVG</h2>\n",
    "      <h3 style=\"margin-top: 0;\">Facultad de Ingenier칤a - Computaci칩n</h3>\n",
    "      <p style=\"font-size: 16px; margin-bottom: 0; margin-top: -20px\">\n",
    "        <strong>Curso:</strong> CC3104 - Aprendizaje por Refuerzo \n",
    "        <strong>Secci칩n:</strong> 10\n",
    "      </p>\n",
    "      <p style=\"font-size: 16px; margin: 0;\"><strong>Laboratorio 4:</strong> M칠todos de Monte Carlo</p>\n",
    "      <br>\n",
    "      <p style=\"font-size: 15px; margin: 0;\"><strong>Autores:</strong></p>\n",
    "      <ul style=\"margin-top: 5px; padding-left: 20px; font-size: 15px;\">\n",
    "        <li>Diego Alexander Hern치ndez Silvestre - <strong>21270</strong></li>\n",
    "        <li>Linda In칠s Jim칠nez Vides - <strong>21169</strong></li>\n",
    "        <li>Mario Antonio Guerra Morales - <strong>21008</strong></li>\n",
    "      </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e0934d",
   "metadata": {},
   "source": [
    "## 游닇 Task 1\n",
    "\n",
    "**1. 쮺칩mo afecta la elecci칩n de la estrategia de exploraci칩n (exploring starts vs soft policy) a la precisi칩n de la evaluaci칩n de pol칤ticas en los m칠todos de Monte Carlo? Considere la posibilidad de comparar el desempe침o de las pol칤ticas evaluadas con y sin explorar los inicios o con diferentes niveles de exploraci칩n en pol칤ticas blandas.**\n",
    "\n",
    "- La elecci칩n de estrategia de exploraci칩n s칤 tiene un impacto directo en la evaluaci칩n de pol칤ticas al usar los m칠todos de Monte Carlo. Si se utiliza Exploring Starts, se garantiza una mayor diversidad en las trayectorias debido a que se puede iniciar desde distintas combinaciones de estados y de acciones. Mientras que, con Soft Policies se a침ade un equilibrio entre la exploraci칩n y la explotaci칩n. Cuando se mantiene una probabilidad de selecci칩n de im치genes aleatorias, se a침ade la posibilidad de seguir explorando nuevas y aleatorias trayectorias, a칰n priorizando acciones de mayor valor.\n",
    "\n",
    "**2. En el contexto del aprendizaje de Monte Carlo fuera de la p칩liza, 쯖칩mo afecta la raz칩n de muestreo de importancia a la convergencia de la evaluaci칩n de pol칤ticas? Explore c칩mo la raz칩n de muestreo de importancia afecta la estabilidad y la convergencia.**\n",
    "\n",
    "- La raz칩n de muestreo de importancia ajusta las estimaciones de retorno para que la pol칤tica objetivo se refleje utilizando datos de manera diferente. Esta afecta al permitirse aprender sobre una pol칤tica objetivo sin hacer un seguimiento directo, pero tambi칠n es capaz de hacer inestables las razones si tanto el comportamiento como el objetivo difieren demasiado.\n",
    "\n",
    "**3. 쮺칩mo puede el uso de una soft policy influir en la eficacia del aprendizaje de pol칤ticas 칩ptimas en comparaci칩n con las pol칤ticas deterministas en los m칠todos de Monte Carlo? Compare el desempe침o y los resultados de aprendizaje de las pol칤ticas derivadas de estrategias 칠psilon-greedy con las derivadas de pol칤ticas deterministas.**\n",
    "\n",
    "- Una soft policy puede influir con una mayor exploraci칩n, esto para ayudar al descubrimiento de acciones que no son efectivas a corto plazo pero s칤 a un largo plazo. Mientras que, una pol칤tica determinista elegir치 la misma acci칩n siempre, lo que puede llegar a darse el riesgo de no hacer una correcta exploraci칩n.\n",
    "\n",
    "**4. 쮺u치les son los posibles beneficios y desventajas de utilizar m칠todos de Monte Carlo off-policy en comparaci칩n con los on-policy en t칠rminos de eficiencia de la muestra, costo computacional y velocidad de aprendizaje?**\n",
    "\n",
    "- Los m칠todos de Monte Carlo off-policy ofrecen una mejor eficiencia al existir la posibilidad de usar datos previos o simulados, pero ello contempla un mayor costo computacional. Sin embargo, esto a su vez permite una gran flexibilidad y con ello una velocidad de aprendizaje considerable, si es que las pol칤ticas no difieren mucho con el objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1376915",
   "metadata": {},
   "source": [
    "## 游닇 Task 2\n",
    "\n",
    "En este ejercicio, simular치 un sistema de gesti칩n de inventarios para una peque침a tienda minorista. La tienda tiene como objetivo maximizar las ganancias manteniendo niveles 칩ptimos de existencias de diferentes productos. Utilizar치 m칠todos de Monte Carlo para la evaluaci칩n de p칩lizas, exploring starts, soft policies y aprendizaje off-policy para estimar el valor de diferentes estrategias de gesti칩n de inventarios. Su objetivo es implementar una soluci칩n en Python y responder preguntas espec칤ficas en funci칩n de los resultados.\n",
    "\n",
    "**Definici칩n del entorno**\n",
    "- Utilice el ambiente dado m치s adelante para simular el entorno de la tienda. Considere que:\n",
    "    - El estado representa los niveles de existencias actuales de los productos.\n",
    "    - Las acciones representan decisiones sobre cu치nto reponer de cada producto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "396c0503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class InventoryEnvironment:\n",
    "    def __init__(self):\n",
    "        self.products = ['product_A', 'product_B']\n",
    "        self.max_stock = 10\n",
    "        self.demand = {'product_A': [0, 1, 2], 'product_B': [0, 1, 2]}\n",
    "        self.restock_cost = {'product_A': 5, 'product_B': 7}\n",
    "        self.sell_price = {'product_A': 10, 'product_B': 15}\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = {product: random.randint(0, self.max_stock) for product in self.products}\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        for product in self.products:\n",
    "            stock = self.state[product]\n",
    "            restock = action[product]\n",
    "            self.state[product] = min(self.max_stock, stock + restock)\n",
    "            demand = random.choice(self.demand[product])\n",
    "            sales = min(demand, self.state[product])\n",
    "            self.state[product] -= sales\n",
    "            reward += sales * self.sell_price[product] - restock * self.restock_cost[product]\n",
    "        return self.state.copy(), reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034ae583",
   "metadata": {},
   "source": [
    "**Generaci칩n de episodios**\n",
    "- Cada episodio representa una serie de d칤as en los que la tienda sigue una pol칤tica de inventario espec칤fica.\n",
    "- Debe recopilar datos para varios episodios y registrar las recompensas (ganancias) de cada d칤a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c17973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, policy, num_days=10):\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "\n",
    "    for _ in range(num_days):\n",
    "        # Elegir acci칩n seg칰n la pol칤tica\n",
    "        action = policy(state)\n",
    "        # Ejecutar acci칩n en el entorno\n",
    "        next_state, reward = env.step(action)\n",
    "        # Guardar transici칩n: estado, acci칩n, recompensa\n",
    "        episode.append((state.copy(), action.copy(), reward))\n",
    "        # Actualizar estado actual\n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9281d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    return {\n",
    "        'product_A': random.randint(0, 3),\n",
    "        'product_B': random.randint(0, 3)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b45f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episodes(env, policy, num_episodes=100, num_days=10):\n",
    "    episodes = []\n",
    "    for _ in range(num_episodes):\n",
    "        episode = generate_episode(env, policy, num_days)\n",
    "        episodes.append(episode)\n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2fe8237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Episodio 1 ---\n",
      "D칤a 1: Estado={'product_A': 10, 'product_B': 10}, Acci칩n={'product_A': 3, 'product_B': 3}, Recompensa=-36\n",
      "D칤a 2: Estado={'product_A': 10, 'product_B': 10}, Acci칩n={'product_A': 0, 'product_B': 1}, Recompensa=43\n",
      "D칤a 3: Estado={'product_A': 8, 'product_B': 8}, Acci칩n={'product_A': 0, 'product_B': 1}, Recompensa=23\n",
      "D칤a 4: Estado={'product_A': 8, 'product_B': 7}, Acci칩n={'product_A': 3, 'product_B': 2}, Recompensa=-9\n",
      "D칤a 5: Estado={'product_A': 8, 'product_B': 9}, Acci칩n={'product_A': 0, 'product_B': 1}, Recompensa=33\n",
      "\n",
      "--- Episodio 2 ---\n",
      "D칤a 1: Estado={'product_A': 1, 'product_B': 3}, Acci칩n={'product_A': 1, 'product_B': 3}, Recompensa=-6\n",
      "D칤a 2: Estado={'product_A': 1, 'product_B': 3}, Acci칩n={'product_A': 0, 'product_B': 3}, Recompensa=4\n",
      "D칤a 3: Estado={'product_A': 0, 'product_B': 5}, Acci칩n={'product_A': 0, 'product_B': 0}, Recompensa=0\n",
      "D칤a 4: Estado={'product_A': 0, 'product_B': 5}, Acci칩n={'product_A': 2, 'product_B': 0}, Recompensa=30\n",
      "D칤a 5: Estado={'product_A': 1, 'product_B': 3}, Acci칩n={'product_A': 3, 'product_B': 2}, Recompensa=11\n",
      "\n",
      "--- Episodio 3 ---\n",
      "D칤a 1: Estado={'product_A': 7, 'product_B': 9}, Acci칩n={'product_A': 3, 'product_B': 2}, Recompensa=-14\n",
      "D칤a 2: Estado={'product_A': 7, 'product_B': 9}, Acci칩n={'product_A': 0, 'product_B': 1}, Recompensa=-7\n",
      "D칤a 3: Estado={'product_A': 7, 'product_B': 10}, Acci칩n={'product_A': 2, 'product_B': 1}, Recompensa=-2\n",
      "D칤a 4: Estado={'product_A': 9, 'product_B': 9}, Acci칩n={'product_A': 2, 'product_B': 1}, Recompensa=23\n",
      "D칤a 5: Estado={'product_A': 9, 'product_B': 8}, Acci칩n={'product_A': 1, 'product_B': 1}, Recompensa=18\n"
     ]
    }
   ],
   "source": [
    "env = InventoryEnvironment()\n",
    "episodes = generate_episodes(env, random_policy, num_episodes=3, num_days=5)\n",
    "\n",
    "for i, ep in enumerate(episodes):\n",
    "    print(f\"\\n--- Episodio {i+1} ---\")\n",
    "    for day, (state, action, reward) in enumerate(ep):\n",
    "        print(f\"D칤a {day+1}: Estado={state}, Acci칩n={action}, Recompensa={reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be23d661",
   "metadata": {},
   "source": [
    "**Exploring Starts**\n",
    "- Implemente explorar inicios para garantizar un conjunto diverso de estados y acciones iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd9d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_exploring_starts(env, policy, num_days=10):\n",
    "    # Estado inicial aleatorio\n",
    "    state = {\n",
    "        product: random.randint(0, env.max_stock)\n",
    "        for product in env.products\n",
    "    }\n",
    "    env.state = state.copy()\n",
    "\n",
    "    episode = []\n",
    "\n",
    "    for day in range(num_days):\n",
    "        # Acci칩n inicial aleatoria solo en el primer paso\n",
    "        if day == 0:\n",
    "            action = {\n",
    "                product: random.randint(0, 3)\n",
    "                for product in env.products\n",
    "            }\n",
    "        else:\n",
    "            action = policy(state)\n",
    "\n",
    "        next_state, reward = env.step(action)\n",
    "        episode.append((state.copy(), action.copy(), reward))\n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f1c72a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episodes_exploring_starts(env, policy, num_episodes=5, num_days=7):\n",
    "    episodes = []\n",
    "    for _ in range(num_episodes):\n",
    "        episode = generate_episode_exploring_starts(env, policy, num_days)\n",
    "        episodes.append(episode)\n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55d95aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Episodio (Exploring Starts) 1 ---\n",
      "D칤a 1: Estado={'product_A': 7, 'product_B': 8}, Acci칩n={'product_A': 0, 'product_B': 3}, Recompensa=-21\n",
      "D칤a 2: Estado={'product_A': 7, 'product_B': 10}, Acci칩n={'product_A': 3, 'product_B': 3}, Recompensa=-26\n",
      "D칤a 3: Estado={'product_A': 9, 'product_B': 10}, Acci칩n={'product_A': 0, 'product_B': 3}, Recompensa=-6\n",
      "D칤a 4: Estado={'product_A': 9, 'product_B': 9}, Acci칩n={'product_A': 0, 'product_B': 2}, Recompensa=21\n",
      "D칤a 5: Estado={'product_A': 7, 'product_B': 9}, Acci칩n={'product_A': 1, 'product_B': 3}, Recompensa=-16\n",
      "\n",
      "--- Episodio (Exploring Starts) 2 ---\n",
      "D칤a 1: Estado={'product_A': 2, 'product_B': 6}, Acci칩n={'product_A': 0, 'product_B': 0}, Recompensa=35\n",
      "D칤a 2: Estado={'product_A': 0, 'product_B': 5}, Acci칩n={'product_A': 2, 'product_B': 2}, Recompensa=-24\n",
      "D칤a 3: Estado={'product_A': 2, 'product_B': 7}, Acci칩n={'product_A': 0, 'product_B': 2}, Recompensa=-4\n",
      "D칤a 4: Estado={'product_A': 1, 'product_B': 9}, Acci칩n={'product_A': 0, 'product_B': 0}, Recompensa=10\n",
      "D칤a 5: Estado={'product_A': 0, 'product_B': 9}, Acci칩n={'product_A': 3, 'product_B': 3}, Recompensa=-36\n",
      "\n",
      "--- Episodio (Exploring Starts) 3 ---\n",
      "D칤a 1: Estado={'product_A': 8, 'product_B': 10}, Acci칩n={'product_A': 1, 'product_B': 1}, Recompensa=-12\n",
      "D칤a 2: Estado={'product_A': 9, 'product_B': 10}, Acci칩n={'product_A': 1, 'product_B': 3}, Recompensa=-26\n",
      "D칤a 3: Estado={'product_A': 10, 'product_B': 10}, Acci칩n={'product_A': 1, 'product_B': 3}, Recompensa=-11\n",
      "D칤a 4: Estado={'product_A': 10, 'product_B': 9}, Acci칩n={'product_A': 2, 'product_B': 3}, Recompensa=9\n",
      "D칤a 5: Estado={'product_A': 9, 'product_B': 8}, Acci칩n={'product_A': 1, 'product_B': 1}, Recompensa=23\n"
     ]
    }
   ],
   "source": [
    "episodes_es = generate_episodes_exploring_starts(env, random_policy, num_episodes=3, num_days=5)\n",
    "\n",
    "for i, ep in enumerate(episodes_es):\n",
    "    print(f\"\\n--- Episodio (Exploring Starts) {i+1} ---\")\n",
    "    for day, (state, action, reward) in enumerate(ep):\n",
    "        print(f\"D칤a {day+1}: Estado={state}, Acci칩n={action}, Recompensa={reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d3fed",
   "metadata": {},
   "source": [
    "**Soft Policies**\n",
    "- Utilice una soft policy (como epsilon-greedy) para garantizar un equilibrio entre la exploraci칩n y la\n",
    "explotaci칩n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5868024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68e209b7",
   "metadata": {},
   "source": [
    "**Aprendizaje off-policy**\n",
    "- Implemente el aprendizaje off-policy para evaluar una pol칤tica objetivo utilizando datos generados\n",
    "por una pol칤tica de comportamiento diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32838af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f86034b",
   "metadata": {},
   "source": [
    "1. 쮺u치l es el valor estimado de mantener diferentes niveles de existencias para cada producto?\n",
    "\n",
    "2. 쮺칩mo afecta el valor epsilon en la pol칤tica blanda al rendimiento?\n",
    "\n",
    "3. 쮺u치l es el impacto de utilizar el aprendizaje fuera de la pol칤tica en comparaci칩n con el aprendizaje dentro de la pol칤tica?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
