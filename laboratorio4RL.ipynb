{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c5ba4e",
   "metadata": {},
   "source": [
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <td style=\"width:20%; vertical-align:middle;\">\n",
    "      <img src=\"LogoUVG.png\" width=\"400\"/>\n",
    "    </td>\n",
    "    <td style=\"text-align:left; vertical-align:middle;\">\n",
    "      <h2 style=\"margin-bottom: 0;\">Universidad del Valle de Guatemala - UVG</h2>\n",
    "      <h3 style=\"margin-top: 0;\">Facultad de Ingeniería - Computación</h3>\n",
    "      <p style=\"font-size: 16px; margin-bottom: 0; margin-top: -20px\">\n",
    "        <strong>Curso:</strong> CC3104 - Aprendizaje por Refuerzo \n",
    "        <strong>Sección:</strong> 10\n",
    "      </p>\n",
    "      <p style=\"font-size: 16px; margin: 0;\"><strong>Laboratorio 4:</strong> Métodos de Monte Carlo</p>\n",
    "      <br>\n",
    "      <p style=\"font-size: 15px; margin: 0;\"><strong>Autores:</strong></p>\n",
    "      <ul style=\"margin-top: 5px; padding-left: 20px; font-size: 15px;\">\n",
    "        <li>Diego Alexander Hernández Silvestre - <strong>21270</strong></li>\n",
    "        <li>Linda Inés Jiménez Vides - <strong>21169</strong></li>\n",
    "        <li>Mario Antonio Guerra Morales - <strong>21008</strong></li>\n",
    "      </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e0934d",
   "metadata": {},
   "source": [
    "## 📝 Task 1\n",
    "\n",
    "**1. ¿Cómo afecta la elección de la estrategia de exploración (exploring starts vs soft policy) a la precisión de la evaluación de políticas en los métodos de Monte Carlo? Considere la posibilidad de comparar el desempeño de las políticas evaluadas con y sin explorar los inicios o con diferentes niveles de exploración en políticas blandas.**\n",
    "\n",
    "- La elección de estrategia de exploración sí tiene un impacto directo en la evaluación de políticas al usar los métodos de Monte Carlo. Si se utiliza Exploring Starts, se garantiza una mayor diversidad en las trayectorias debido a que se puede iniciar desde distintas combinaciones de estados y de acciones. Mientras que, con Soft Policies se añade un equilibrio entre la exploración y la explotación. Cuando se mantiene una probabilidad de selección de imágenes aleatorias, se añade la posibilidad de seguir explorando nuevas y aleatorias trayectorias, aún priorizando acciones de mayor valor.\n",
    "\n",
    "**2. En el contexto del aprendizaje de Monte Carlo fuera de la póliza, ¿cómo afecta la razón de muestreo de importancia a la convergencia de la evaluación de políticas? Explore cómo la razón de muestreo de importancia afecta la estabilidad y la convergencia.**\n",
    "\n",
    "- La razón de muestreo de importancia ajusta las estimaciones de retorno para que la política objetivo se refleje utilizando datos de manera diferente. Esta afecta al permitirse aprender sobre una política objetivo sin hacer un seguimiento directo, pero también es capaz de hacer inestables las razones si tanto el comportamiento como el objetivo difieren demasiado.\n",
    "\n",
    "**3. ¿Cómo puede el uso de una soft policy influir en la eficacia del aprendizaje de políticas óptimas en comparación con las políticas deterministas en los métodos de Monte Carlo? Compare el desempeño y los resultados de aprendizaje de las políticas derivadas de estrategias épsilon-greedy con las derivadas de políticas deterministas.**\n",
    "\n",
    "- Una soft policy puede influir con una mayor exploración, esto para ayudar al descubrimiento de acciones que no son efectivas a corto plazo pero sí a un largo plazo. Mientras que, una política determinista elegirá la misma acción siempre, lo que puede llegar a darse el riesgo de no hacer una correcta exploración.\n",
    "\n",
    "**4. ¿Cuáles son los posibles beneficios y desventajas de utilizar métodos de Monte Carlo off-policy en comparación con los on-policy en términos de eficiencia de la muestra, costo computacional y velocidad de aprendizaje?**\n",
    "\n",
    "- Los métodos de Monte Carlo off-policy ofrecen una mejor eficiencia al existir la posibilidad de usar datos previos o simulados, pero ello contempla un mayor costo computacional. Sin embargo, esto a su vez permite una gran flexibilidad y con ello una velocidad de aprendizaje considerable, si es que las políticas no difieren mucho con el objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1376915",
   "metadata": {},
   "source": [
    "## 📝 Task 2\n",
    "\n",
    "En este ejercicio, simulará un sistema de gestión de inventarios para una pequeña tienda minorista. La tienda tiene como objetivo maximizar las ganancias manteniendo niveles óptimos de existencias de diferentes productos. Utilizará métodos de Monte Carlo para la evaluación de pólizas, exploring starts, soft policies y aprendizaje off-policy para estimar el valor de diferentes estrategias de gestión de inventarios. Su objetivo es implementar una solución en Python y responder preguntas específicas en función de los resultados.\n",
    "\n",
    "**Definición del entorno**\n",
    "- Utilice el ambiente dado más adelante para simular el entorno de la tienda. Considere que:\n",
    "    - El estado representa los niveles de existencias actuales de los productos.\n",
    "    - Las acciones representan decisiones sobre cuánto reponer de cada producto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "396c0503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class InventoryEnvironment:\n",
    "    def __init__(self):\n",
    "        self.products = ['product_A', 'product_B']\n",
    "        self.max_stock = 10\n",
    "        self.demand = {'product_A': [0, 1, 2], 'product_B': [0, 1, 2]}\n",
    "        self.restock_cost = {'product_A': 5, 'product_B': 7}\n",
    "        self.sell_price = {'product_A': 10, 'product_B': 15}\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = {product: random.randint(0, self.max_stock) for product in self.products}\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        for product in self.products:\n",
    "            stock = self.state[product]\n",
    "            restock = action[product]\n",
    "            self.state[product] = min(self.max_stock, stock + restock)\n",
    "            demand = random.choice(self.demand[product])\n",
    "            sales = min(demand, self.state[product])\n",
    "            self.state[product] -= sales\n",
    "            reward += sales * self.sell_price[product] - restock * self.restock_cost[product]\n",
    "        return self.state.copy(), reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034ae583",
   "metadata": {},
   "source": [
    "**Generación de episodios**\n",
    "- Cada episodio representa una serie de días en los que la tienda sigue una política de inventario específica.\n",
    "- Debe recopilar datos para varios episodios y registrar las recompensas (ganancias) de cada día"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c17973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, policy, num_days=10):\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "\n",
    "    for _ in range(num_days):\n",
    "        # Elegir acción según la política\n",
    "        action = policy(state)\n",
    "        # Ejecutar acción en el entorno\n",
    "        next_state, reward = env.step(action)\n",
    "        # Guardar transición: estado, acción, recompensa\n",
    "        episode.append((state.copy(), action.copy(), reward))\n",
    "        # Actualizar estado actual\n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9281d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    return {\n",
    "        'product_A': random.randint(0, 3),\n",
    "        'product_B': random.randint(0, 3)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b45f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episodes(env, policy, num_episodes=100, num_days=10):\n",
    "    episodes = []\n",
    "    for _ in range(num_episodes):\n",
    "        episode = generate_episode(env, policy, num_days)\n",
    "        episodes.append(episode)\n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2fe8237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Episodio 1 ---\n",
      "Día 1: Estado={'product_A': 2, 'product_B': 2}, Acción={'product_A': 1, 'product_B': 0}, Recompensa=15\n",
      "Día 2: Estado={'product_A': 2, 'product_B': 2}, Acción={'product_A': 0, 'product_B': 1}, Recompensa=43\n",
      "Día 3: Estado={'product_A': 0, 'product_B': 1}, Acción={'product_A': 2, 'product_B': 2}, Recompensa=26\n",
      "Día 4: Estado={'product_A': 0, 'product_B': 1}, Acción={'product_A': 3, 'product_B': 3}, Recompensa=14\n",
      "Día 5: Estado={'product_A': 1, 'product_B': 2}, Acción={'product_A': 0, 'product_B': 0}, Recompensa=25\n",
      "\n",
      "--- Episodio 2 ---\n",
      "Día 1: Estado={'product_A': 2, 'product_B': 10}, Acción={'product_A': 2, 'product_B': 3}, Recompensa=-31\n",
      "Día 2: Estado={'product_A': 2, 'product_B': 10}, Acción={'product_A': 3, 'product_B': 1}, Recompensa=3\n",
      "Día 3: Estado={'product_A': 4, 'product_B': 9}, Acción={'product_A': 1, 'product_B': 3}, Recompensa=-6\n",
      "Día 4: Estado={'product_A': 3, 'product_B': 10}, Acción={'product_A': 0, 'product_B': 1}, Recompensa=43\n",
      "Día 5: Estado={'product_A': 1, 'product_B': 8}, Acción={'product_A': 0, 'product_B': 3}, Recompensa=-11\n",
      "\n",
      "--- Episodio 3 ---\n",
      "Día 1: Estado={'product_A': 0, 'product_B': 9}, Acción={'product_A': 0, 'product_B': 3}, Recompensa=-6\n",
      "Día 2: Estado={'product_A': 0, 'product_B': 9}, Acción={'product_A': 3, 'product_B': 3}, Recompensa=14\n",
      "Día 3: Estado={'product_A': 1, 'product_B': 8}, Acción={'product_A': 2, 'product_B': 0}, Recompensa=5\n",
      "Día 4: Estado={'product_A': 3, 'product_B': 7}, Acción={'product_A': 1, 'product_B': 1}, Recompensa=8\n",
      "Día 5: Estado={'product_A': 2, 'product_B': 8}, Acción={'product_A': 1, 'product_B': 0}, Recompensa=45\n"
     ]
    }
   ],
   "source": [
    "env = InventoryEnvironment()\n",
    "episodes = generate_episodes(env, random_policy, num_episodes=3, num_days=5)\n",
    "\n",
    "for i, ep in enumerate(episodes):\n",
    "    print(f\"\\n--- Episodio {i+1} ---\")\n",
    "    for day, (state, action, reward) in enumerate(ep):\n",
    "        print(f\"Día {day+1}: Estado={state}, Acción={action}, Recompensa={reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be23d661",
   "metadata": {},
   "source": [
    "**Exploring Starts**\n",
    "- Implemente explorar inicios para garantizar un conjunto diverso de estados y acciones iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd9d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_exploring_starts(env, policy, num_days=10):\n",
    "    # Estado inicial aleatorio\n",
    "    state = {\n",
    "        product: random.randint(0, env.max_stock)\n",
    "        for product in env.products\n",
    "    }\n",
    "    env.state = state.copy()\n",
    "\n",
    "    episode = []\n",
    "\n",
    "    for day in range(num_days):\n",
    "        # Acción inicial aleatoria solo en el primer paso\n",
    "        if day == 0:\n",
    "            action = {\n",
    "                product: random.randint(0, 3)\n",
    "                for product in env.products\n",
    "            }\n",
    "        else:\n",
    "            action = policy(state)\n",
    "\n",
    "        next_state, reward = env.step(action)\n",
    "        episode.append((state.copy(), action.copy(), reward))\n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f1c72a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episodes_exploring_starts(env, policy, num_episodes=5, num_days=7):\n",
    "    episodes = []\n",
    "    for _ in range(num_episodes):\n",
    "        episode = generate_episode_exploring_starts(env, policy, num_days)\n",
    "        episodes.append(episode)\n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55d95aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Episodio (Exploring Starts) 1 ---\n",
      "Día 1: Estado={'product_A': 1, 'product_B': 0}, Acción={'product_A': 3, 'product_B': 3}, Recompensa=4\n",
      "Día 2: Estado={'product_A': 3, 'product_B': 1}, Acción={'product_A': 2, 'product_B': 1}, Recompensa=18\n",
      "Día 3: Estado={'product_A': 3, 'product_B': 1}, Acción={'product_A': 2, 'product_B': 1}, Recompensa=13\n",
      "Día 4: Estado={'product_A': 5, 'product_B': 0}, Acción={'product_A': 1, 'product_B': 1}, Recompensa=3\n",
      "Día 5: Estado={'product_A': 6, 'product_B': 0}, Acción={'product_A': 0, 'product_B': 0}, Recompensa=20\n",
      "\n",
      "--- Episodio (Exploring Starts) 2 ---\n",
      "Día 1: Estado={'product_A': 2, 'product_B': 6}, Acción={'product_A': 3, 'product_B': 1}, Recompensa=-2\n",
      "Día 2: Estado={'product_A': 3, 'product_B': 7}, Acción={'product_A': 0, 'product_B': 2}, Recompensa=36\n",
      "Día 3: Estado={'product_A': 1, 'product_B': 7}, Acción={'product_A': 2, 'product_B': 1}, Recompensa=33\n",
      "Día 4: Estado={'product_A': 1, 'product_B': 6}, Acción={'product_A': 1, 'product_B': 0}, Recompensa=10\n",
      "Día 5: Estado={'product_A': 2, 'product_B': 5}, Acción={'product_A': 3, 'product_B': 2}, Recompensa=-14\n",
      "\n",
      "--- Episodio (Exploring Starts) 3 ---\n",
      "Día 1: Estado={'product_A': 9, 'product_B': 1}, Acción={'product_A': 1, 'product_B': 3}, Recompensa=-6\n",
      "Día 2: Estado={'product_A': 8, 'product_B': 4}, Acción={'product_A': 0, 'product_B': 0}, Recompensa=35\n",
      "Día 3: Estado={'product_A': 6, 'product_B': 3}, Acción={'product_A': 3, 'product_B': 3}, Recompensa=-36\n",
      "Día 4: Estado={'product_A': 9, 'product_B': 6}, Acción={'product_A': 0, 'product_B': 2}, Recompensa=21\n",
      "Día 5: Estado={'product_A': 7, 'product_B': 7}, Acción={'product_A': 3, 'product_B': 3}, Recompensa=4\n"
     ]
    }
   ],
   "source": [
    "episodes_es = generate_episodes_exploring_starts(env, random_policy, num_episodes=3, num_days=5)\n",
    "\n",
    "for i, ep in enumerate(episodes_es):\n",
    "    print(f\"\\n--- Episodio (Exploring Starts) {i+1} ---\")\n",
    "    for day, (state, action, reward) in enumerate(ep):\n",
    "        print(f\"Día {day+1}: Estado={state}, Acción={action}, Recompensa={reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d3fed",
   "metadata": {},
   "source": [
    "**Soft Policies**\n",
    "- Utilice una soft policy (como epsilon-greedy) para garantizar un equilibrio entre la exploración y la\n",
    "explotación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5868024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Episodio (Soft Policy) 1 ---\n",
      "Día 1: Estado={'product_A': 2, 'product_B': 2}, Acción={'product_A': 1, 'product_B': 0}, Recompensa=15\n",
      "Día 2: Estado={'product_A': 2, 'product_B': 2}, Acción={'product_A': 0, 'product_B': 1}, Recompensa=43\n",
      "Día 3: Estado={'product_A': 0, 'product_B': 1}, Acción={'product_A': 2, 'product_B': 2}, Recompensa=26\n",
      "Día 4: Estado={'product_A': 0, 'product_B': 1}, Acción={'product_A': 3, 'product_B': 3}, Recompensa=14\n",
      "Día 5: Estado={'product_A': 1, 'product_B': 2}, Acción={'product_A': 0, 'product_B': 0}, Recompensa=25\n",
      "\n",
      "--- Episodio (Soft Policy) 2 ---\n",
      "Día 1: Estado={'product_A': 2, 'product_B': 10}, Acción={'product_A': 2, 'product_B': 3}, Recompensa=-31\n",
      "Día 2: Estado={'product_A': 2, 'product_B': 10}, Acción={'product_A': 3, 'product_B': 1}, Recompensa=3\n",
      "Día 3: Estado={'product_A': 4, 'product_B': 9}, Acción={'product_A': 1, 'product_B': 3}, Recompensa=-6\n",
      "Día 4: Estado={'product_A': 3, 'product_B': 10}, Acción={'product_A': 0, 'product_B': 1}, Recompensa=43\n",
      "Día 5: Estado={'product_A': 1, 'product_B': 8}, Acción={'product_A': 0, 'product_B': 3}, Recompensa=-11\n",
      "\n",
      "--- Episodio (Soft Policy) 3 ---\n",
      "Día 1: Estado={'product_A': 0, 'product_B': 9}, Acción={'product_A': 0, 'product_B': 3}, Recompensa=-6\n",
      "Día 2: Estado={'product_A': 0, 'product_B': 9}, Acción={'product_A': 3, 'product_B': 3}, Recompensa=14\n",
      "Día 3: Estado={'product_A': 1, 'product_B': 8}, Acción={'product_A': 2, 'product_B': 0}, Recompensa=5\n",
      "Día 4: Estado={'product_A': 3, 'product_B': 7}, Acción={'product_A': 1, 'product_B': 1}, Recompensa=8\n",
      "Día 5: Estado={'product_A': 2, 'product_B': 8}, Acción={'product_A': 1, 'product_B': 0}, Recompensa=45\n",
      "\n",
      "--- Episodio (Soft Policy) 4 ---\n",
      "Día 1: Estado={'product_A': 10, 'product_B': 1}, Acción={'product_A': 3, 'product_B': 0}, Recompensa=0\n",
      "Día 2: Estado={'product_A': 10, 'product_B': 1}, Acción={'product_A': 3, 'product_B': 1}, Recompensa=28\n",
      "Día 3: Estado={'product_A': 8, 'product_B': 0}, Acción={'product_A': 1, 'product_B': 3}, Recompensa=-16\n",
      "Día 4: Estado={'product_A': 8, 'product_B': 3}, Acción={'product_A': 2, 'product_B': 3}, Recompensa=-1\n",
      "Día 5: Estado={'product_A': 10, 'product_B': 4}, Acción={'product_A': 0, 'product_B': 0}, Recompensa=10\n",
      "\n",
      "--- Episodio (Soft Policy) 5 ---\n",
      "Día 1: Estado={'product_A': 2, 'product_B': 2}, Acción={'product_A': 1, 'product_B': 2}, Recompensa=16\n",
      "Día 2: Estado={'product_A': 2, 'product_B': 2}, Acción={'product_A': 0, 'product_B': 1}, Recompensa=23\n",
      "Día 3: Estado={'product_A': 2, 'product_B': 1}, Acción={'product_A': 2, 'product_B': 3}, Recompensa=-16\n",
      "Día 4: Estado={'product_A': 4, 'product_B': 3}, Acción={'product_A': 0, 'product_B': 0}, Recompensa=20\n",
      "Día 5: Estado={'product_A': 2, 'product_B': 3}, Acción={'product_A': 0, 'product_B': 3}, Recompensa=9\n",
      "\n",
      "--- Episodio (Soft Policy) 6 ---\n",
      "Día 1: Estado={'product_A': 4, 'product_B': 5}, Acción={'product_A': 3, 'product_B': 3}, Recompensa=-36\n",
      "Día 2: Estado={'product_A': 4, 'product_B': 5}, Acción={'product_A': 2, 'product_B': 0}, Recompensa=0\n",
      "Día 3: Estado={'product_A': 5, 'product_B': 5}, Acción={'product_A': 2, 'product_B': 1}, Recompensa=3\n",
      "Día 4: Estado={'product_A': 5, 'product_B': 6}, Acción={'product_A': 3, 'product_B': 1}, Recompensa=3\n",
      "Día 5: Estado={'product_A': 7, 'product_B': 6}, Acción={'product_A': 1, 'product_B': 2}, Recompensa=21\n",
      "\n",
      "--- Episodio (Soft Policy) 7 ---\n",
      "Día 1: Estado={'product_A': 8, 'product_B': 9}, Acción={'product_A': 1, 'product_B': 3}, Recompensa=-1\n",
      "Día 2: Estado={'product_A': 8, 'product_B': 9}, Acción={'product_A': 0, 'product_B': 1}, Recompensa=28\n",
      "Día 3: Estado={'product_A': 6, 'product_B': 9}, Acción={'product_A': 3, 'product_B': 3}, Recompensa=-6\n",
      "Día 4: Estado={'product_A': 9, 'product_B': 8}, Acción={'product_A': 1, 'product_B': 2}, Recompensa=1\n",
      "Día 5: Estado={'product_A': 8, 'product_B': 10}, Acción={'product_A': 0, 'product_B': 2}, Recompensa=-4\n",
      "\n",
      "--- Episodio (Soft Policy) 8 ---\n",
      "Día 1: Estado={'product_A': 6, 'product_B': 9}, Acción={'product_A': 3, 'product_B': 2}, Recompensa=-14\n",
      "Día 2: Estado={'product_A': 6, 'product_B': 9}, Acción={'product_A': 1, 'product_B': 3}, Recompensa=-11\n",
      "Día 3: Estado={'product_A': 7, 'product_B': 9}, Acción={'product_A': 2, 'product_B': 2}, Recompensa=-14\n",
      "Día 4: Estado={'product_A': 8, 'product_B': 10}, Acción={'product_A': 3, 'product_B': 0}, Recompensa=15\n",
      "Día 5: Estado={'product_A': 10, 'product_B': 8}, Acción={'product_A': 3, 'product_B': 1}, Recompensa=3\n"
     ]
    }
   ],
   "source": [
    "Q = {} # Q-table para almacenar valores de acción-estado = [list of returns]\n",
    "\n",
    "def epsilon_greedy_policy(Q, state, epsilon=0.1):\n",
    "    state_key = str(state)\n",
    "    if state_key not in Q or random.random() < epsilon:\n",
    "        # Seleccionar acción aleatoria\n",
    "        return {\n",
    "            'product_A': random.randint(0, 3),\n",
    "            'product_B': random.randint(0, 3)\n",
    "        }\n",
    "    else:\n",
    "        # Seleccionar la mejor acción basada en Q\n",
    "        best_action = max(Q[state_key], key=lambda a: np.mean(Q[state_key][a]))\n",
    "        return eval(best_action)\n",
    "\n",
    "def generate_episodes_soft_policy(env, Q, num_episodes=5, num_days=7, epsilon=0.1):\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode = []\n",
    "\n",
    "        for _ in range(num_days):\n",
    "            action = epsilon_greedy_policy(Q, state, epsilon)\n",
    "            next_state, reward = env.step(action)\n",
    "            episode.append((state.copy(), action.copy(), reward))\n",
    "            state = next_state\n",
    "\n",
    "        episodes.append(episode)\n",
    "    return episodes\n",
    "\n",
    "def update_Q(Q, episodes):\n",
    "    for episode in episodes:\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for state, action, reward in reversed(episode):\n",
    "            G += reward\n",
    "            state_key = str(state)\n",
    "            action_key = str(action)\n",
    "\n",
    "            if (state_key, action_key) not in visited:\n",
    "                visited.add((state_key, action_key))\n",
    "                if state_key not in Q:\n",
    "                    Q[state_key] = {}\n",
    "                if action_key not in Q[state_key]:\n",
    "                    Q[state_key][action_key] = []\n",
    "                Q[state_key][action_key].append(G)\n",
    "\n",
    "\n",
    "Q = {}\n",
    "episodes_soft = generate_episodes_soft_policy(env, Q, epsilon=0.1, num_episodes=5, num_days=5)\n",
    "update_Q(Q, episodes_soft)\n",
    "\n",
    "# Ver resultados\n",
    "for i, ep in enumerate(episodes_soft):\n",
    "    print(f\"\\n--- Episodio (Soft Policy) {i+1} ---\")\n",
    "    for day, (state, action, reward) in enumerate(ep):\n",
    "        print(f\"Día {day+1}: Estado={state}, Acción={action}, Recompensa={reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e209b7",
   "metadata": {},
   "source": [
    "**Aprendizaje off-policy**\n",
    "- Implemente el aprendizaje off-policy para evaluar una política objetivo utilizando datos generados\n",
    "por una política de comportamiento diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32838af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f86034b",
   "metadata": {},
   "source": [
    "1. ¿Cuál es el valor estimado de mantener diferentes niveles de existencias para cada producto?\n",
    "\n",
    "2. ¿Cómo afecta el valor epsilon en la política blanda al rendimiento?\n",
    "\n",
    "3. ¿Cuál es el impacto de utilizar el aprendizaje fuera de la política en comparación con el aprendizaje dentro de la política?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
