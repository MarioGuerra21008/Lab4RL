{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c5ba4e",
   "metadata": {},
   "source": [
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <td style=\"width:20%; vertical-align:middle;\">\n",
    "      <img src=\"LogoUVG.png\" width=\"400\"/>\n",
    "    </td>\n",
    "    <td style=\"text-align:left; vertical-align:middle;\">\n",
    "      <h2 style=\"margin-bottom: 0;\">Universidad del Valle de Guatemala - UVG</h2>\n",
    "      <h3 style=\"margin-top: 0;\">Facultad de Ingeniería - Computación</h3>\n",
    "      <p style=\"font-size: 16px; margin-bottom: 0; margin-top: -20px\">\n",
    "        <strong>Curso:</strong> CC3104 - Aprendizaje por Refuerzo \n",
    "        <strong>Sección:</strong> 10\n",
    "      </p>\n",
    "      <p style=\"font-size: 16px; margin: 0;\"><strong>Laboratorio 4:</strong> Métodos de Monte Carlo</p>\n",
    "      <br>\n",
    "      <p style=\"font-size: 15px; margin: 0;\"><strong>Autores:</strong></p>\n",
    "      <ul style=\"margin-top: 5px; padding-left: 20px; font-size: 15px;\">\n",
    "        <li>Diego Alexander Hernández Silvestre - <strong>21270</strong></li>\n",
    "        <li>Linda Inés Jiménez Vides - <strong>21169</strong></li>\n",
    "        <li>Mario Antonio Guerra Morales - <strong>21008</strong></li>\n",
    "      </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e0934d",
   "metadata": {},
   "source": [
    "## 📝 Task 1\n",
    "\n",
    "**1. ¿Cómo afecta la elección de la estrategia de exploración (exploring starts vs soft policy) a la precisión de la evaluación de políticas en los métodos de Monte Carlo? Considere la posibilidad de comparar el desempeño de las políticas evaluadas con y sin explorar los inicios o con diferentes niveles de exploración en políticas blandas.**\n",
    "\n",
    "- La elección de estrategia de exploración sí tiene un impacto directo en la evaluación de políticas al usar los métodos de Monte Carlo. Si se utiliza Exploring Starts, se garantiza una mayor diversidad en las trayectorias debido a que se puede iniciar desde distintas combinaciones de estados y de acciones. Mientras que, con Soft Policies se añade un equilibrio entre la exploración y la explotación. Cuando se mantiene una probabilidad de selección de imágenes aleatorias, se añade la posibilidad de seguir explorando nuevas y aleatorias trayectorias, aún priorizando acciones de mayor valor.\n",
    "\n",
    "**2. En el contexto del aprendizaje de Monte Carlo fuera de la póliza, ¿cómo afecta la razón de muestreo de importancia a la convergencia de la evaluación de políticas? Explore cómo la razón de muestreo de importancia afecta la estabilidad y la convergencia.**\n",
    "\n",
    "- La razón de muestreo de importancia ajusta las estimaciones de retorno para que la política objetivo se refleje utilizando datos de manera diferente. Esta afecta al permitirse aprender sobre una política objetivo sin hacer un seguimiento directo, pero también es capaz de hacer inestables las razones si tanto el comportamiento como el objetivo difieren demasiado.\n",
    "\n",
    "**3. ¿Cómo puede el uso de una soft policy influir en la eficacia del aprendizaje de políticas óptimas en comparación con las políticas deterministas en los métodos de Monte Carlo? Compare el desempeño y los resultados de aprendizaje de las políticas derivadas de estrategias épsilon-greedy con las derivadas de políticas deterministas.**\n",
    "\n",
    "- Una soft policy puede influir con una mayor exploración, esto para ayudar al descubrimiento de acciones que no son efectivas a corto plazo pero sí a un largo plazo. Mientras que, una política determinista elegirá la misma acción siempre, lo que puede llegar a darse el riesgo de no hacer una correcta exploración.\n",
    "\n",
    "**4. ¿Cuáles son los posibles beneficios y desventajas de utilizar métodos de Monte Carlo off-policy en comparación con los on-policy en términos de eficiencia de la muestra, costo computacional y velocidad de aprendizaje?**\n",
    "\n",
    "- Los métodos de Monte Carlo off-policy ofrecen una mejor eficiencia al existir la posibilidad de usar datos previos o simulados, pero ello contempla un mayor costo computacional. Sin embargo, esto a su vez permite una gran flexibilidad y con ello una velocidad de aprendizaje considerable, si es que las políticas no difieren mucho con el objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1376915",
   "metadata": {},
   "source": [
    "## 📝 Task 2\n",
    "\n",
    "En este ejercicio, simulará un sistema de gestión de inventarios para una pequeña tienda minorista. La tienda tiene como objetivo maximizar las ganancias manteniendo niveles óptimos de existencias de diferentes productos. Utilizará métodos de Monte Carlo para la evaluación de pólizas, exploring starts, soft policies y aprendizaje off-policy para estimar el valor de diferentes estrategias de gestión de inventarios. Su objetivo es implementar una solución en Python y responder preguntas específicas en función de los resultados.\n",
    "\n",
    "**Definición del entorno**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c0503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class InventoryEnvironment:\n",
    "    def __init__(self):\n",
    "        self.products = ['product_A', 'product_B']\n",
    "        self.max_stock = 10\n",
    "        self.demand = {'product_A': [0, 1, 2], 'product_B': [0, 1, 2]}\n",
    "        self.restock_cost = {'product_A': 5, 'product_B': 7}\n",
    "        self.sell_price = {'product_A': 10, 'product_B': 15}\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = {product: random.randint(0, self.max_stock) for product in self.products}\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        for product in self.products:\n",
    "            stock = self.state[product]\n",
    "            restock = action[product]\n",
    "            self.state[product] = min(self.max_stock, stock + restock)\n",
    "            demand = random.choice(self.demand[product])\n",
    "            sales = min(demand, self.state[product])\n",
    "            self.state[product] -= sales\n",
    "            reward += sales * self.sell_price[product] - restock * self.restock_cost[product]\n",
    "        return self.state.copy(), reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034ae583",
   "metadata": {},
   "source": [
    "**Generación de episodios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9281d98a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be23d661",
   "metadata": {},
   "source": [
    "**Exploring Starts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9d198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c9d3fed",
   "metadata": {},
   "source": [
    "**Soft Policies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5868024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68e209b7",
   "metadata": {},
   "source": [
    "**Aprendizaje off-policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32838af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f86034b",
   "metadata": {},
   "source": [
    "1. ¿Cuál es el valor estimado de mantener diferentes niveles de existencias para cada producto?\n",
    "\n",
    "2. ¿Cómo afecta el valor epsilon en la política blanda al rendimiento?\n",
    "\n",
    "3. ¿Cuál es el impacto de utilizar el aprendizaje fuera de la política en comparación con el aprendizaje dentro de la política?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
